{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90642671-d626-4465-827a-51efb7aed89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47555d0e-3772-48de-acb4-572925d38534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/woojay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/woojay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7dc5ed-5331-4f9c-94ee-838e5faa7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database():\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect('../data/arxiv_papers.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Query all entries in the 'papers' table\n",
    "    cursor.execute(\"SELECT paper_id, title, summary, updated FROM papers\")\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Construct the list of dictionaries\n",
    "    articles = [\n",
    "        {'id': row[0], 'title': row[1], 'summary': row[2], 'date': row[3]} for row in rows\n",
    "    ]\n",
    "    \n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06315648-d1df-46a6-8911-6a31eb47b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = query_database()\n",
    "# Convert articles to a DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Preprocess summaries: tokenize, lower, remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['processed_summary'] = df['summary'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.isalpha() and word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2f991f-727f-452b-bb30-4ab4c705384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency analysis\n",
    "all_words = [word for summary in df['processed_summary'] for word in summary]\n",
    "word_freq = Counter(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb93b91-8d9c-456c-9aef-61d427a5185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier manipulation\n",
    "freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'frequency']).sort_values(by='frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68cc23d-732c-4d43-8446-d300aae95bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend analysis: Count word occurrences by year\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "trends = df.explode('processed_summary').groupby(['year', 'processed_summary']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065abc0d-a428-4070-ace9-02e009cf9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results to SQLite database\n",
    "conn = sqlite3.connect('../data/arxiv_analysis.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS word_frequency (\n",
    "    word TEXT PRIMARY KEY,\n",
    "    frequency INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS word_trends (\n",
    "    year INTEGER,\n",
    "    word TEXT,\n",
    "    count INTEGER,\n",
    "    PRIMARY KEY (year, word)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert frequency analysis results\n",
    "for _, row in freq_df.iterrows():\n",
    "    cursor.execute('REPLACE INTO word_frequency (word, frequency) VALUES (?, ?)', (row['word'], row['frequency']))\n",
    "\n",
    "# Insert trend analysis results\n",
    "for _, row in trends.iterrows():\n",
    "    cursor.execute('REPLACE INTO word_trends (year, word, count) VALUES (?, ?, ?)', (row['year'], row['processed_summary'], row['counts']))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba6a50e-a988-4623-a6cd-e5d3c684af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency and trend analysis data saved to database.\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequency and trend analysis data saved to database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7534b2-6d94-40d8-87e6-8abfdc29204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('../data/arxiv_analysis.db')\n",
    "\n",
    "TOP_K=200\n",
    "# Load the data from the database\n",
    "word_freq_desc = pd.read_sql_query(f\"SELECT * FROM word_frequency ORDER BY frequency DESC LIMIT {TOP_K}\", conn)\n",
    "word_freq_asc = pd.read_sql_query(f\"SELECT * FROM word_frequency ORDER BY frequency ASC LIMIT {TOP_K}\", conn)\n",
    "word_trends = pd.read_sql_query(\"SELECT * FROM word_trends WHERE word IN (SELECT word FROM word_frequency ORDER BY frequency DESC LIMIT 10)\", conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2780f8f7-e22f-4876-b362-76d5b785ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  frequency\n",
      "0  delve         27\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/arxiv_analysis.db')\n",
    "\n",
    "print(pd.read_sql_query(\"SELECT * FROM word_frequency WHERE word = 'delve'\", conn))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4f2057d-05be-4937-b0fe-70e8e3a3848a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3561622799.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[48], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.tight_layout()%%!\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "\n",
    "# Top TOP_K most frequent words visualization\n",
    "plt.figure(figsize=(12, 28))\n",
    "sns.barplot(x='frequency', y='word', data=word_freq_desc, palette='viridis')\n",
    "plt.title(f'Top {TOP_K} Most Frequent Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.tight_layout()%%!\n",
    "plt.show()\n",
    "\n",
    "# Top TOP_K lease frequent words visualization\n",
    "plt.figure(figsize=(12, 28))\n",
    "sns.barplot(x='frequency', y='word', data=word_freq_asc, palette='viridis')\n",
    "plt.title(f'Top {TOP_K} Lease Frequent Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trends of top 10 most frequent words over years\n",
    "plt.figure(figsize=(12, 8))\n",
    "for word in word_trends['word'].unique():\n",
    "    subset = word_trends[word_trends['word'] == word]\n",
    "    plt.plot(subset['year'], subset['count'], marker='o', label=word)\n",
    "\n",
    "plt.title('Trend Analysis of Top 10 Words Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Word')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a218ea-c788-4d70-8496-45e60f6eb80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>updated</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2403.19925v1</td>\n",
       "      <td>Decision Mamba: Reinforcement Learning via Seq...</td>\n",
       "      <td>Decision Transformer, a promising approach t...</td>\n",
       "      <td>2024-03-29T02:25:55Z</td>\n",
       "      <td>The study explores enhancing the Decision Tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2403.19444v1</td>\n",
       "      <td>Transparent and Clinically Interpretable AI fo...</td>\n",
       "      <td>The rapidly advancing field of Explainable A...</td>\n",
       "      <td>2024-03-28T14:15:13Z</td>\n",
       "      <td>Explainable Artificial Intelligence (XAI) addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2403.18969v1</td>\n",
       "      <td>A Survey on Large Language Models from Concept...</td>\n",
       "      <td>Recent advancements in Large Language Models...</td>\n",
       "      <td>2024-03-27T19:35:41Z</td>\n",
       "      <td>Recent advancements in Large Language Models (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2403.18637v1</td>\n",
       "      <td>Transformers-based architectures for stroke se...</td>\n",
       "      <td>Stroke remains a significant global health c...</td>\n",
       "      <td>2024-03-27T14:42:08Z</td>\n",
       "      <td>This review explores how deep learning techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2403.18985v1</td>\n",
       "      <td>Robustness and Visual Explanation for Black Bo...</td>\n",
       "      <td>We present a generic Reinforcement Learning ...</td>\n",
       "      <td>2024-03-27T20:07:39Z</td>\n",
       "      <td>A Reinforcement Learning framework is proposed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>http://arxiv.org/abs/2404.07883v1</td>\n",
       "      <td>Apprentice Tutor Builder: A Platform For Users...</td>\n",
       "      <td>Intelligent tutoring systems (ITS) are effec...</td>\n",
       "      <td>2024-04-11T16:14:23Z</td>\n",
       "      <td>The Apprentice Tutor Builder (ATB) simplifies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>http://arxiv.org/abs/2404.07709v1</td>\n",
       "      <td>A Geometrical Analysis of Kernel Ridge Regress...</td>\n",
       "      <td>We obtain upper bounds for the estimation er...</td>\n",
       "      <td>2024-04-11T12:53:23Z</td>\n",
       "      <td>The text presents upper bounds for the estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>http://arxiv.org/abs/2404.07479v1</td>\n",
       "      <td>RASSAR: Room Accessibility and Safety Scanning...</td>\n",
       "      <td>The safety and accessibility of our homes is...</td>\n",
       "      <td>2024-04-11T05:12:13Z</td>\n",
       "      <td>Researchers have developed RASSAR, a mobile AR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>http://arxiv.org/abs/2404.07898v1</td>\n",
       "      <td>Anomaly Detection in Power Grids via Context-A...</td>\n",
       "      <td>An important tool grid operators use to safe...</td>\n",
       "      <td>2024-04-11T16:37:01Z</td>\n",
       "      <td>Grid operators use SCADA data to detect anomal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>http://arxiv.org/abs/2404.07509v1</td>\n",
       "      <td>Multiparameter cascaded quantum interferometer</td>\n",
       "      <td>We theoretically propose a multiparameter ca...</td>\n",
       "      <td>2024-04-11T06:58:57Z</td>\n",
       "      <td>A proposed multiparameter cascaded quantum int...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              paper_id  \\\n",
       "0    http://arxiv.org/abs/2403.19925v1   \n",
       "1    http://arxiv.org/abs/2403.19444v1   \n",
       "2    http://arxiv.org/abs/2403.18969v1   \n",
       "3    http://arxiv.org/abs/2403.18637v1   \n",
       "4    http://arxiv.org/abs/2403.18985v1   \n",
       "..                                 ...   \n",
       "309  http://arxiv.org/abs/2404.07883v1   \n",
       "310  http://arxiv.org/abs/2404.07709v1   \n",
       "311  http://arxiv.org/abs/2404.07479v1   \n",
       "312  http://arxiv.org/abs/2404.07898v1   \n",
       "313  http://arxiv.org/abs/2404.07509v1   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Decision Mamba: Reinforcement Learning via Seq...   \n",
       "1    Transparent and Clinically Interpretable AI fo...   \n",
       "2    A Survey on Large Language Models from Concept...   \n",
       "3    Transformers-based architectures for stroke se...   \n",
       "4    Robustness and Visual Explanation for Black Bo...   \n",
       "..                                                 ...   \n",
       "309  Apprentice Tutor Builder: A Platform For Users...   \n",
       "310  A Geometrical Analysis of Kernel Ridge Regress...   \n",
       "311  RASSAR: Room Accessibility and Safety Scanning...   \n",
       "312  Anomaly Detection in Power Grids via Context-A...   \n",
       "313     Multiparameter cascaded quantum interferometer   \n",
       "\n",
       "                                               summary               updated  \\\n",
       "0      Decision Transformer, a promising approach t...  2024-03-29T02:25:55Z   \n",
       "1      The rapidly advancing field of Explainable A...  2024-03-28T14:15:13Z   \n",
       "2      Recent advancements in Large Language Models...  2024-03-27T19:35:41Z   \n",
       "3      Stroke remains a significant global health c...  2024-03-27T14:42:08Z   \n",
       "4      We present a generic Reinforcement Learning ...  2024-03-27T20:07:39Z   \n",
       "..                                                 ...                   ...   \n",
       "309    Intelligent tutoring systems (ITS) are effec...  2024-04-11T16:14:23Z   \n",
       "310    We obtain upper bounds for the estimation er...  2024-04-11T12:53:23Z   \n",
       "311    The safety and accessibility of our homes is...  2024-04-11T05:12:13Z   \n",
       "312    An important tool grid operators use to safe...  2024-04-11T16:37:01Z   \n",
       "313    We theoretically propose a multiparameter ca...  2024-04-11T06:58:57Z   \n",
       "\n",
       "                                       concise_summary  \n",
       "0    The study explores enhancing the Decision Tran...  \n",
       "1    Explainable Artificial Intelligence (XAI) addr...  \n",
       "2    Recent advancements in Large Language Models (...  \n",
       "3    This review explores how deep learning techniq...  \n",
       "4    A Reinforcement Learning framework is proposed...  \n",
       "..                                                 ...  \n",
       "309  The Apprentice Tutor Builder (ATB) simplifies ...  \n",
       "310  The text presents upper bounds for the estimat...  \n",
       "311  Researchers have developed RASSAR, a mobile AR...  \n",
       "312  Grid operators use SCADA data to detect anomal...  \n",
       "313  A proposed multiparameter cascaded quantum int...  \n",
       "\n",
       "[314 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/arxiv_papers.db')\n",
    "\n",
    "tool = pd.read_sql_query(f\"SELECT * FROM papers WHERE summary LIKE '%tool%'\", conn)\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67c6372-fd44-4cd6-ae86-144acaee5675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153be94c-17fd-4b46-ab12-e9fc980e02b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "http://arxiv.org/abs/2404.05086v1\n",
      "LoRA is a popular method for adapting Large Language Models (LLMs) efficiently, with this note providing additional perspectives and insights for deploying LoRA at scale. The aim is to enhance the understanding and application of LoRA without introducing new experiments.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.03114v1\n",
      "Large Language Models (LLMs) excel at code generation and understanding, but their performance can be significantly impacted by the quality of accompanying documentation. Incorrect documentation can impair an LLM's ability to understand code, while incomplete or missing documentation has a less pronounced effect on code comprehension by LLMs.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.05446v1\n",
      "Large Language Models (LLMs) face limitations due to small context window sizes, prompting efforts to expand them to accommodate up to 200K input tokens. To address the need for comprehensive evaluation and understanding of long-context tasks, a new benchmark called XL$^2$Bench is introduced, which includes tasks with long-range dependencies and longer text lengths, showcasing that current LLMs still fall short of human performance levels on such demanding benchmarks.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.04566v1\n",
      "Large Language Models (LLMs) have advanced software engineering tasks, leading to the emergence of the Large Language Models for Software Engineering (LLM4SE) field. This position paper advocates for prioritizing efficient and environmentally friendly LLM4SE solutions, proposing a roadmap for future research to achieve this goal and revolutionize the software engineering landscape.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.02637v1\n",
      "The rapid progress of Large Language Models (LLMs) is leading to an increase in both their applications and the number of attackers seeking to exploit them for revealing sensitive information or spreading false data. By inserting carefully selected words from the model's vocabulary, a novel approach has been developed to manipulate LLMs effectively and inconspicuously, even using a different model than the one being targeted for the attack.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.06395v1\n",
      "The text discusses the potential of Small Language Models (SLMs) as a cost-effective alternative to Large Language Models (LLMs), introducing MiniCPM models that perform well compared to larger LLMs. Through the use of a Warmup-Stable-Decay learning rate scheduler, the MiniCPM models demonstrate efficient data-model scaling and offer scalability for future LLM research.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.04392v1\n",
      "Large Language Models (LLMs) are widely used in various applications but are susceptible to attacks like jailbreaking and privacy leakage. Fine-tuning and quantization of foundational LLMs can enhance performance but may increase vulnerabilities, emphasizing the importance of external guardrails to mitigate risks.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.05829v1\n",
      "This paper explores adapting pre-trained LLMs to new languages by extending vocabulary, optimizing preferences, and addressing data scarcity. The study includes experiments across 9 languages and 2 parameter scales, outperforming existing baselines and making evaluation code and checkpoints publicly available for further research.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.06480v2\n",
      "The LLM community is focusing on improving models' ability to handle lengthy documents, leading to a need for precise evaluation metrics. Current evaluation benchmarks like L-Eval and LongBench are limited in assessing models across different lengths, prompting the introduction of Ada-LEval, a flexible benchmark for evaluating long-context understanding of LLMs with subsets enabling reliable assessments up to 128k tokens.\n",
      "----------\n",
      "http://arxiv.org/abs/2404.02837v1\n",
      "A study on large language models (LLMs) reveals that a small subset of key parameters significantly impacts model performance, while most parameters have minimal effect, leading to the proposal of CherryQ, a quantization method that optimizes parameter precision. CherryQ demonstrates superior performance in quantization compared to existing methods, enabling efficient deployment of LLMs by leveraging parameter heterogeneity.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:8000/query\"\n",
    "data = {\n",
    "    \"top_k\": 10,\n",
    "    \"query_text\": \"tool use in llm or large language model\"\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "articles = json.loads(response.text)\n",
    "\n",
    "for article in articles:\n",
    "    print('-'*10)\n",
    "    print(article[\"id\"])\n",
    "    print(article[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2264b4-dfda-4395-9387-33d47d51baf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1572b-5c52-4edf-8145-7f0f49743dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
