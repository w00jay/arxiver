{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c040e712-d339-43cb-bdff-03680a5a2918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 20:11:08.831791: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-15 20:11:08.862027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-15 20:11:08.862051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-15 20:11:08.862729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 20:11:08.867692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 20:11:09.478178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from chromadb.utils import embedding_functions\n",
    "from keras_tuner import RandomSearch\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f98c9b-ad5b-43e0-b7ab-4e4a66276d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CUTOFF = 3003  # 2711 2424 2155 1880 1572 1016 817 502 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34115fc5-9f10-4615-8fc6-54f1206885da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_interested_entry(database_path):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQL query to find the last occurrence of \"interested\" = 1\n",
    "    query = \"\"\"\n",
    "    SELECT paper_id FROM papers \n",
    "    WHERE interested = 1 \n",
    "    ORDER BY paper_id ASC \n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        last_interested = cursor.fetchone()\n",
    "        if last_interested:\n",
    "            print(\"Last interested entry:\", last_interested)\n",
    "            return {\"paper_id\": last_interested[0]}\n",
    "        else:\n",
    "            print(\"No interested entries found.\")\n",
    "            return nil\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe783ae0-0c05-4ec7-8e9b-33d972deab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last interested entry: ('http://arxiv.org/abs/1706.03762v7',)\n",
      "{\n",
      "    \"paper_id\": \"http://arxiv.org/abs/1706.03762v7\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "db = \"../data/arxiv_papers.db\"\n",
    "last_interested = find_last_interested_entry(db)\n",
    "print(json.dumps(last_interested, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3218c5-a977-4691-b11f-760b7e620bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(database_path):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.row_factory = sqlite3.Row\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT paper_id, concise_summary, interested FROM papers \n",
    "    ORDER BY paper_id ASC \n",
    "    LIMIT {TRAIN_CUTOFF};\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        articles = cursor.fetchall()\n",
    "        if articles:\n",
    "            print(f\"Got {len(articles)}.\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(\"No interested entries found.\")\n",
    "            return nil\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115b2d07-576a-4260-9165-4d2772b009f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(paper_id, vdb_path=\"../data/arxiv_embeddings.chroma\"):\n",
    "    vdb = chromadb.PersistentClient(vdb_path)\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    embedding_func = sentence_transformer_ef\n",
    "    vectors = vdb.get_or_create_collection(\n",
    "        name=\"arxiver\", embedding_function=embedding_func\n",
    "    )\n",
    "\n",
    "    res = vectors.get(ids=[paper_id], limit=1, include=[\"embeddings\"])\n",
    "    # print(res)\n",
    "    # print(res[\"embeddings\"][0])\n",
    "    # print(f'{res[\"ids\"][0]} {res[\"embeddings\"][0]}')\n",
    "    return res[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3abb7b26-c040-411e-800c-80254d8680c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woojay/P/ML/.direnv/python-3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/woojay/P/ML/.direnv/python-3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3003, 384) (3003,)\n"
     ]
    }
   ],
   "source": [
    "articles = get_data(db)\n",
    "X_article = []\n",
    "y_article = []\n",
    "for article in articles:\n",
    "    # print(\n",
    "    #     f'{article[\"paper_id\"]}, {article[\"interested\"]}\\n{article[\"concise_summary\"]}'\n",
    "    # )\n",
    "    embedding = get_embedding(article[\"paper_id\"])\n",
    "    interested = article[\"interested\"]\n",
    "\n",
    "    if np.any(np.isnan(embedding)):\n",
    "        print(f'{article[\"paper_id\"]} embedding is NaN: {embedding}')\n",
    "    if len(embedding) == 0:\n",
    "        print(f'{article[\"paper_id\"]} embedding is empty')\n",
    "    # if interested.dtype == \"object\":\n",
    "    #     print(f'{article[\"paper_id\"]} embedding is object: {interested}')\n",
    "    X_article.append(embedding)\n",
    "    y_article.append(interested)\n",
    "\n",
    "# print(X_article[:3])\n",
    "# print(y_article[:3])\n",
    "\n",
    "X = np.array(X_article)\n",
    "y = np.array(y_article)\n",
    "\n",
    "# print(X[:1])\n",
    "# print(y[:1])\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc22c976-1746-4844-8dfd-d194e6c26da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0028905950020998716,\n",
       " -0.05982412025332451,\n",
       " -0.008419168181717396,\n",
       " -0.01921667717397213,\n",
       " 0.014117264188826084,\n",
       " 0.031683020293712616,\n",
       " -0.04353299364447594,\n",
       " 0.07392609864473343,\n",
       " 0.07040511816740036,\n",
       " -0.05495050922036171,\n",
       " -0.011163090355694294,\n",
       " -0.010397370904684067,\n",
       " 0.04608140140771866,\n",
       " 0.009848348796367645,\n",
       " -0.010315321385860443,\n",
       " 0.013171806000173092,\n",
       " 0.0007975147454999387,\n",
       " 0.023529132828116417,\n",
       " -0.08075197786092758,\n",
       " -0.06419838964939117,\n",
       " 0.040608182549476624,\n",
       " 0.04551651328802109,\n",
       " 0.03515498712658882,\n",
       " -0.011132987216114998,\n",
       " 0.03609246760606766,\n",
       " 0.014492835849523544,\n",
       " -0.04349182918667793,\n",
       " -0.07201968133449554,\n",
       " -0.03566231578588486,\n",
       " -0.0035466880071908236,\n",
       " -0.0017921729013323784,\n",
       " -1.6525307728443295e-05,\n",
       " -0.003994284197688103,\n",
       " 0.11520082503557205,\n",
       " -0.042225077748298645,\n",
       " 0.021955693140625954,\n",
       " -0.0885772779583931,\n",
       " 0.0006367009482346475,\n",
       " -0.0018035508692264557,\n",
       " -0.04127293825149536,\n",
       " -0.03715188056230545,\n",
       " -0.04181300103664398,\n",
       " 0.004717599134892225,\n",
       " 0.020079748705029488,\n",
       " 0.12638629972934723,\n",
       " 0.011757900007069111,\n",
       " 0.013007208704948425,\n",
       " 0.011290333233773708,\n",
       " 0.04141438007354736,\n",
       " 0.014196956530213356,\n",
       " -0.09675116091966629,\n",
       " -0.042060814797878265,\n",
       " 0.012644651345908642,\n",
       " 0.057601045817136765,\n",
       " -0.014588892459869385,\n",
       " 0.05498352274298668,\n",
       " 0.006518681533634663,\n",
       " 0.03475780412554741,\n",
       " -0.004166896920651197,\n",
       " -0.0547633096575737,\n",
       " -0.06210476905107498,\n",
       " -0.018257195129990578,\n",
       " -0.027639886364340782,\n",
       " -0.012833566404879093,\n",
       " 0.03485867753624916,\n",
       " -0.012836847454309464,\n",
       " 0.01864645443856716,\n",
       " -0.01383234467357397,\n",
       " -0.10308153927326202,\n",
       " -0.013363003730773926,\n",
       " -0.008456509560346603,\n",
       " 0.05769491568207741,\n",
       " 0.014770819805562496,\n",
       " 0.08211624622344971,\n",
       " 0.0766645297408104,\n",
       " 0.025611035525798798,\n",
       " 0.010705484077334404,\n",
       " 0.02064969204366207,\n",
       " 0.04949244111776352,\n",
       " -0.02814926952123642,\n",
       " 0.023817570880055428,\n",
       " -0.01734309270977974,\n",
       " 0.10586445778608322,\n",
       " 0.032064151018857956,\n",
       " 0.09077880531549454,\n",
       " -0.058521151542663574,\n",
       " 0.04660880193114281,\n",
       " 0.03993644937872887,\n",
       " -0.04132290929555893,\n",
       " -0.00570293515920639,\n",
       " -0.05313454195857048,\n",
       " -0.09578235447406769,\n",
       " 0.10896845161914825,\n",
       " 0.030625859275460243,\n",
       " -0.0386124849319458,\n",
       " 0.039430346339941025,\n",
       " -0.02810133621096611,\n",
       " -0.033900488168001175,\n",
       " -0.05214226245880127,\n",
       " 0.028485054150223732,\n",
       " -0.03289109468460083,\n",
       " 0.03164559602737427,\n",
       " 0.04005889967083931,\n",
       " -0.04602324217557907,\n",
       " -0.07989565283060074,\n",
       " -0.0009359910618513823,\n",
       " 0.07038108259439468,\n",
       " 0.030079830437898636,\n",
       " 0.06147569790482521,\n",
       " -0.10333587229251862,\n",
       " 0.04233980178833008,\n",
       " 0.007875493727624416,\n",
       " -0.01761510968208313,\n",
       " -0.07361552119255066,\n",
       " 0.06073106825351715,\n",
       " -0.08448058366775513,\n",
       " -0.0004528039426077157,\n",
       " -0.04509928822517395,\n",
       " 0.022964410483837128,\n",
       " 0.02608148567378521,\n",
       " -0.059468913823366165,\n",
       " 0.060292020440101624,\n",
       " -0.037970785051584244,\n",
       " 0.009102054871618748,\n",
       " 0.006651001982390881,\n",
       " -0.00798786710947752,\n",
       " 0.011957261711359024,\n",
       " 2.5165236528372927e-33,\n",
       " 0.02581857517361641,\n",
       " 0.0823797732591629,\n",
       " 0.0284844059497118,\n",
       " -0.007864858023822308,\n",
       " -0.0011038660304620862,\n",
       " -0.02199939824640751,\n",
       " -0.026142528280615807,\n",
       " 0.05926758423447609,\n",
       " -0.062602199614048,\n",
       " -0.07133104652166367,\n",
       " -0.04836658015847206,\n",
       " -0.03397873044013977,\n",
       " -0.014098023995757103,\n",
       " 0.05687042698264122,\n",
       " -0.046055782586336136,\n",
       " -0.0670304074883461,\n",
       " 0.03613744303584099,\n",
       " 0.06256747990846634,\n",
       " -0.003219716949388385,\n",
       " 0.007038390263915062,\n",
       " -0.001788127003237605,\n",
       " 0.0024882876314222813,\n",
       " 0.07505842298269272,\n",
       " -0.018026553094387054,\n",
       " 0.038580309599637985,\n",
       " -0.01872418448328972,\n",
       " 0.07932816445827484,\n",
       " -0.08097332715988159,\n",
       " -0.03731519728899002,\n",
       " -0.040234215557575226,\n",
       " -0.11377225816249847,\n",
       " -0.010386548936367035,\n",
       " -0.0024661184288561344,\n",
       " -0.01689758524298668,\n",
       " -0.006094949785619974,\n",
       " -0.09949196875095367,\n",
       " 0.007345006801187992,\n",
       " -0.021560614928603172,\n",
       " 0.035761334002017975,\n",
       " -0.060414332896471024,\n",
       " 0.0069123525172472,\n",
       " 0.04494389519095421,\n",
       " -0.013454418629407883,\n",
       " 0.06919834762811661,\n",
       " -0.04331161826848984,\n",
       " 0.004497344605624676,\n",
       " -0.035546641796827316,\n",
       " 0.02130165882408619,\n",
       " 0.038706567138433456,\n",
       " 0.002912509487941861,\n",
       " 0.021952755749225616,\n",
       " -0.012775328941643238,\n",
       " -0.0706549808382988,\n",
       " -0.0008192004170268774,\n",
       " 0.14082065224647522,\n",
       " 0.10919435322284698,\n",
       " 0.08978234976530075,\n",
       " 0.06376867741346359,\n",
       " 0.029147319495677948,\n",
       " -0.014399693347513676,\n",
       " 0.06926445662975311,\n",
       " 0.022194435819983482,\n",
       " 0.029614677652716637,\n",
       " 0.10733402520418167,\n",
       " 0.08786888420581818,\n",
       " -0.013682455755770206,\n",
       " -0.02477351203560829,\n",
       " 0.043912407010793686,\n",
       " 0.026661992073059082,\n",
       " -0.01982146129012108,\n",
       " -0.10243168473243713,\n",
       " -0.04090846702456474,\n",
       " 0.019132958725094795,\n",
       " -0.01799425482749939,\n",
       " 0.032766666263341904,\n",
       " 0.039304908365011215,\n",
       " -0.03843339905142784,\n",
       " -0.09827830642461777,\n",
       " 0.023127039894461632,\n",
       " -0.017350582405924797,\n",
       " -0.05373495817184448,\n",
       " -0.05983688682317734,\n",
       " 0.020376145839691162,\n",
       " -0.02197766676545143,\n",
       " -0.014613676816225052,\n",
       " 0.034862954169511795,\n",
       " 0.08134778589010239,\n",
       " -0.04964283108711243,\n",
       " 0.03425081446766853,\n",
       " 0.05430987849831581,\n",
       " 0.007954113185405731,\n",
       " 0.0023705866187810898,\n",
       " -0.10456972569227219,\n",
       " 0.022812355309724808,\n",
       " 0.017031779512763023,\n",
       " -4.796296131263253e-34,\n",
       " -0.05020258203148842,\n",
       " 0.013796033337712288,\n",
       " -0.11083831638097763,\n",
       " 0.07449213415384293,\n",
       " -0.015627939254045486,\n",
       " -0.05493006110191345,\n",
       " 0.0074884421192109585,\n",
       " -1.392148533341242e-05,\n",
       " -0.01828034780919552,\n",
       " 0.02225387468934059,\n",
       " 0.003474305383861065,\n",
       " -0.08403202146291733,\n",
       " 0.05502568185329437,\n",
       " -0.029674895107746124,\n",
       " 0.024385983124375343,\n",
       " -0.025049323216080666,\n",
       " 0.03823332116007805,\n",
       " 0.033692535012960434,\n",
       " -0.021262360736727715,\n",
       " 0.03806140646338463,\n",
       " -0.0458279512822628,\n",
       " 0.1076563224196434,\n",
       " -0.03481616824865341,\n",
       " 0.013495235703885555,\n",
       " -0.0657079890370369,\n",
       " -0.024796826764941216,\n",
       " -0.08048851042985916,\n",
       " 0.09278421849012375,\n",
       " -0.005915334448218346,\n",
       " -0.025416476652026176,\n",
       " -0.02760729379951954,\n",
       " -0.01883764937520027,\n",
       " -0.0013625622959807515,\n",
       " -0.006969819776713848,\n",
       " -0.0352175235748291,\n",
       " 0.09222719818353653,\n",
       " -0.015587574802339077,\n",
       " -0.025410177186131477,\n",
       " -0.0013889383990317583,\n",
       " 0.08626660704612732,\n",
       " 0.0302477665245533,\n",
       " -0.03115040250122547,\n",
       " -0.07790698856115341,\n",
       " 0.012668192386627197,\n",
       " -0.03032251074910164,\n",
       " 0.010595801286399364,\n",
       " -0.06629183143377304,\n",
       " -0.05759246647357941,\n",
       " -0.0806887075304985,\n",
       " 0.06706751883029938,\n",
       " -0.016519663855433464,\n",
       " 0.05443289130926132,\n",
       " -0.09512640535831451,\n",
       " -0.005788971204310656,\n",
       " 0.01710953563451767,\n",
       " -0.06744519621133804,\n",
       " 0.002820655470713973,\n",
       " -0.11469588428735733,\n",
       " 0.023759180679917336,\n",
       " -0.07132615149021149,\n",
       " -0.01152228657156229,\n",
       " 0.00253187189809978,\n",
       " 0.09542519599199295,\n",
       " -0.05729883536696434,\n",
       " 0.029876528307795525,\n",
       " -0.03361739218235016,\n",
       " 0.016422713175415993,\n",
       " 0.02572963573038578,\n",
       " 0.07007206231355667,\n",
       " 0.026846079155802727,\n",
       " 0.08938849717378616,\n",
       " -0.044983871281147,\n",
       " 0.04989135265350342,\n",
       " 0.09656819701194763,\n",
       " 0.0025089753326028585,\n",
       " -0.0503481887280941,\n",
       " 0.05605089291930199,\n",
       " -0.03094080276787281,\n",
       " -0.012704077176749706,\n",
       " -0.08784835040569305,\n",
       " -0.058938492089509964,\n",
       " 0.024570347741246223,\n",
       " 0.004714910872280598,\n",
       " -0.006849863566458225,\n",
       " 0.04557708650827408,\n",
       " 0.03507419303059578,\n",
       " 0.11560025811195374,\n",
       " 0.01753433234989643,\n",
       " 0.08356227725744247,\n",
       " 0.04243360459804535,\n",
       " 0.030097542330622673,\n",
       " 0.07830106467008591,\n",
       " 0.0019216955406591296,\n",
       " 0.047605715692043304,\n",
       " -0.057128746062517166,\n",
       " -3.832630568467721e-08,\n",
       " -0.13467082381248474,\n",
       " 0.05680611729621887,\n",
       " -0.07750553637742996,\n",
       " 0.057495612651109695,\n",
       " 0.0014966120943427086,\n",
       " -0.1353536993265152,\n",
       " -0.038904037326574326,\n",
       " 0.07061344385147095,\n",
       " 0.007386479992419481,\n",
       " 0.0027843602001667023,\n",
       " -0.004012160934507847,\n",
       " -0.06161164864897728,\n",
       " -0.062478601932525635,\n",
       " -0.06454472988843918,\n",
       " 0.058466773480176926,\n",
       " 0.06014438346028328,\n",
       " 0.07677017152309418,\n",
       " 0.04895259067416191,\n",
       " 0.01756599172949791,\n",
       " 0.02159186638891697,\n",
       " 0.020403876900672913,\n",
       " 0.051930561661720276,\n",
       " 0.0025324206799268723,\n",
       " 0.003166384994983673,\n",
       " 0.02638803794980049,\n",
       " -0.02905111014842987,\n",
       " -0.05093410238623619,\n",
       " -0.030604036524891853,\n",
       " 0.02279680036008358,\n",
       " -0.07476367801427841,\n",
       " 0.033352117985486984,\n",
       " 0.030942166224122047,\n",
       " 0.009441320784389973,\n",
       " -0.03638001158833504,\n",
       " 0.04586214944720268,\n",
       " 0.10565483570098877,\n",
       " 0.05580025911331177,\n",
       " 0.05097644031047821,\n",
       " 0.04594162106513977,\n",
       " 0.06616783887147903,\n",
       " 0.07614389061927795,\n",
       " -0.013218332082033157,\n",
       " -0.08438953012228012,\n",
       " 0.055680517107248306,\n",
       " -0.05342602729797363,\n",
       " -0.047794412821531296,\n",
       " -0.06236325576901436,\n",
       " -0.0622422955930233,\n",
       " 0.05747947841882706,\n",
       " -0.09390784800052643,\n",
       " 0.11438514292240143,\n",
       " -0.03989621251821518,\n",
       " -0.02858893945813179,\n",
       " 0.011010554619133472,\n",
       " 0.009946401230990887,\n",
       " 0.025340154767036438,\n",
       " 0.049351975321769714,\n",
       " -0.05128622427582741,\n",
       " 0.08521975576877594,\n",
       " 0.050311338156461716,\n",
       " 0.022854430601000786,\n",
       " 0.060489870607852936,\n",
       " -0.060847047716379166,\n",
       " -0.04929362237453461]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(articles[0][\"paper_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05805479-ad97-46da-907d-90957faac493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 1\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]), y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e62a7967-49b5-48ee-9f5e-4b03a83bf28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2402, 384) (2402,) (601, 384) (601,)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# # Convert data types\n",
    "\n",
    "if y_train.dtype == object:\n",
    "    y_train = y_train.astype(float)\n",
    "\n",
    "# X_train = X_train.astype('float32')\n",
    "# y_train = y_train.astype('int32')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069c1ea5-bd39-46d8-8229-759cd8e84385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of X_train: float64\n",
      "Data type of y_train: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of X_train:\", X_train.dtype)\n",
    "print(\"Data type of y_train:\", y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5deac1-cfc3-43f0-9aad-bcb0c71a3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in X_train: False\n",
      "NaN in y_train: False\n",
      "Infinite in X_train: True\n",
      "Infinite in y_train: True\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN in X_train:\", np.any(np.isnan(X_train)))\n",
    "print(\"NaN in y_train:\", np.any(np.isnan(y_train)))\n",
    "print(\"Infinite in X_train:\", np.all(np.isfinite(X_train)))\n",
    "print(\"Infinite in y_train:\", np.all(np.isfinite(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4624ccb4-0a03-48fc-b910-557dfb5d9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential(\n",
    "    # [\n",
    "    #     Dense(384, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    #     Dropout(0.2),\n",
    "    #     Dense(64, activation=\"relu\"),\n",
    "    #     Dense(1, activation=\"sigmoid\"),\n",
    "    # ]\n",
    "    [\n",
    "        Dense(384, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        Dense(224, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532f9ae5-0a9e-429b-a1f8-d442f3582efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "61/61 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.9438 - val_loss: 0.1826 - val_accuracy: 0.9480\n",
      "Epoch 2/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9500 - val_loss: 0.1880 - val_accuracy: 0.9480\n",
      "Epoch 3/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.9500 - val_loss: 0.1972 - val_accuracy: 0.9480\n",
      "Epoch 4/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.9500 - val_loss: 0.2203 - val_accuracy: 0.9480\n",
      "Epoch 5/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9558 - val_loss: 0.2786 - val_accuracy: 0.9459\n",
      "Epoch 6/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9755 - val_loss: 0.3280 - val_accuracy: 0.9439\n",
      "Epoch 7/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9865 - val_loss: 0.3560 - val_accuracy: 0.9356\n",
      "Epoch 8/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.9958 - val_loss: 0.4075 - val_accuracy: 0.9397\n",
      "Epoch 9/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9995 - val_loss: 0.4540 - val_accuracy: 0.9231\n",
      "Epoch 10/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4912 - val_accuracy: 0.9356\n",
      "Epoch 11/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.5373 - val_accuracy: 0.9418\n",
      "Epoch 12/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9990 - val_loss: 0.5148 - val_accuracy: 0.9335\n",
      "Epoch 13/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.5451 - val_accuracy: 0.9314\n",
      "Epoch 14/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.5339 - val_accuracy: 0.9272\n",
      "Epoch 15/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.5581 - val_accuracy: 0.9314\n",
      "Epoch 16/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.5767 - val_accuracy: 0.9252\n",
      "Epoch 17/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 0.6141 - val_accuracy: 0.9480\n",
      "Epoch 18/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.5649 - val_accuracy: 0.9314\n",
      "Epoch 19/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.5956 - val_accuracy: 0.9397\n",
      "Epoch 20/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 0.5893 - val_accuracy: 0.9376\n",
      "Epoch 21/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 5.4949e-04 - accuracy: 1.0000 - val_loss: 0.5825 - val_accuracy: 0.9314\n",
      "Epoch 22/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 3.1285e-04 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 0.9335\n",
      "Epoch 23/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.1664e-04 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 0.9376\n",
      "Epoch 24/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 2.0764e-04 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 0.9335\n",
      "Epoch 25/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.4236e-04 - accuracy: 1.0000 - val_loss: 0.6352 - val_accuracy: 0.9335\n",
      "Epoch 26/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.5082e-04 - accuracy: 1.0000 - val_loss: 0.6435 - val_accuracy: 0.9335\n",
      "Epoch 27/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.3779e-04 - accuracy: 1.0000 - val_loss: 0.6500 - val_accuracy: 0.9335\n",
      "Epoch 28/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 1.1009e-04 - accuracy: 1.0000 - val_loss: 0.6593 - val_accuracy: 0.9335\n",
      "Epoch 29/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 9.6243e-05 - accuracy: 1.0000 - val_loss: 0.6641 - val_accuracy: 0.9335\n",
      "Epoch 30/30\n",
      "61/61 [==============================] - 0s 3ms/step - loss: 9.1895e-05 - accuracy: 1.0000 - val_loss: 0.6691 - val_accuracy: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc6b5d95210>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "473d8456-003d-48ca-aec8-fa6c8f99bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import datetime\n",
    "\n",
    "formatted_time = datetime.datetime.now().strftime(f\"%Y%m%d_%H%M\")\n",
    "model.save(f\"model-{formatted_time}-{TRAIN_CUTOFF}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a43cedba-e5ff-413e-b2e1-89c2dfd1e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 951us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       567\n",
      "           1       0.29      0.12      0.17        34\n",
      "\n",
      "    accuracy                           0.93       601\n",
      "   macro avg       0.62      0.55      0.57       601\n",
      "weighted avg       0.91      0.93      0.92       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "predictions = model.predict(X_test) > 0.5\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ab900e-d34b-4920-adf1-fb1c32f99f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data(database_path):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.row_factory = sqlite3.Row\n",
    "\n",
    "    # SQL query to find the last occurrence of \"interested\" = 1\n",
    "    query = f\"\"\"\n",
    "    SELECT paper_id, concise_summary FROM papers\n",
    "    ORDER BY paper_id ASC\n",
    "    LIMIT 2000 OFFSET {TRAIN_CUTOFF};\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        articles = cursor.fetchall()\n",
    "        if articles:\n",
    "            print(f\"Got {len(articles)}.\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(\"No interested entries found.\")\n",
    "            return nil\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f93e5fb3-87a4-40ee-be94-0eda3ae02a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/abs/2404.04234v2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ids = get_new_data(db)\n",
    "\n",
    "new_ids[0][\"paper_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6dbb32a-31dc-44f0-b399-95c974a909f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 996us/step\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted articles\n",
    "\n",
    "new_x = []\n",
    "formatted = []\n",
    "for id in new_ids:\n",
    "    new_x.append(get_embedding(id[\"paper_id\"]))\n",
    "\n",
    "new_preds = model.predict(new_x) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e3d6c35-587d-44d0-953e-809e78bf047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2404.04286v1: [ True]\n",
      "Advancements in Large Language Models (LLMs) are leading to increased iterative interactions between models, with multi-round self-improving methods allowing for new example generation. By drawing parallels between LLM behavior and human cultural evolution using a Bayesian framework like Iterated Learning (IL), researchers aim to predict and guide the evolution of LLMs towards desired outcomes based on experimental verification.\n",
      "http://arxiv.org/abs/2404.04298v1: [ True]\n",
      "The study investigates whether Language Models (LLMs) can enhance their performance by refining previous outputs. Despite introducing a framework to evaluate generative and discriminative abilities, the experimental analysis of various LLMs suggests that they do not consistently excel in discrimination over initial generation, which could impact the advancement of self-improving AI systems.\n",
      "http://arxiv.org/abs/2404.04361v1: [ True]\n",
      "The study examines Large Language Models' (LLMs) effectiveness in predicting entity-specific sentiment from political news articles using zero-shot and few-shot strategies. The research highlights LLMs' superior performance over fine-tuned BERT models in capturing entity-specific sentiment, emphasizing the significance of suitable prompting strategies and model architectures in entity-centric sentiment analysis within the political news domain.\n",
      "http://arxiv.org/abs/2404.04442v1: [ True]\n",
      "Large Language Models (LLMs) are advancing artificial intelligence by enabling autonomous agents with human-like text skills, poised to revolutionize industries like healthcare and customer service. Despite challenges like multimodality and value alignment, techniques such as prompting and reasoning are being explored to enhance these agents, with evaluation platforms like AgentBench and ToolLLM facilitating robust assessment in complex scenarios, paving the way for more capable autonomous agents that are anticipated to become integral in digital tasks from email responses to disease diagnosis.\n",
      "http://arxiv.org/abs/2404.04570v1: [ True]\n",
      "This text discusses the impact of large language models on AI system interaction patterns and highlights the lack of focus on human interaction with LLM in current literature reviews. The authors conducted a detailed review and mapping of 110 relevant publications, offering insights into human-LLM interaction patterns and identifying key challenges in this area.\n",
      "http://arxiv.org/abs/2404.04750v2: [ True]\n",
      "The advancements in artificial intelligence present both opportunities and challenges for society, with the potential to significantly impact humanity akin to the industrial revolution. The One Hundred Year Study on AI, founded by multidisciplinary experts, emphasizes the importance of understanding AI models, addressing societal impacts, and fostering collaboration among stakeholders to guide the development of AI responsibly for human benefit.\n",
      "http://arxiv.org/abs/2404.04821v1: [ True]\n",
      "AI is reshaping Software Engineering with a focus on Automated Software Evolution for intelligent applications, presenting complexity from data heterogeneity and changing contexts. A proposed conceptual framework emphasizes multimodality learning and offers a Selective Sequential Scope Model (3S) for categorizing research across SE phases, serving as a practical guide for practitioners venturing into this evolving domain.\n",
      "http://arxiv.org/abs/2404.04834v1: [ True]\n",
      "This paper discusses the integration of Large Language Models (LLMs) into autonomous agents to enhance cognitive abilities in addressing complex software engineering challenges through LLM-based Multi-Agent (LMA) systems, emphasizing benefits like collaborative problem-solving and scalability. It envisions the role of LMA systems in future software engineering practices, highlighting potential applications, emerging challenges, and research opportunities to guide future directions.\n",
      "http://arxiv.org/abs/2404.04839v1: [ True]\n",
      "DevOps and security integration into the workflow is crucial, with AI-driven security approaches showing promise in automating security tasks to ensure uninterrupted delivery speed. A comprehensive analysis of AI-driven security techniques applicable to DevOps has identified key challenges and opportunities for enhancing security, trust, and efficiency in software development processes.\n",
      "http://arxiv.org/abs/2404.04949v1: [ True]\n",
      "Large language models are being used in specialized fields, facing challenges with diverse data leading to conflicts during task transfer. The Adaptive Semantic Space Learning framework improves multi-expert model performance by reorganizing data distributions, as shown by the financial LLM \"SilverSight\" achieving high results with minimal data and strong generalization abilities.\n",
      "http://arxiv.org/abs/2404.05417v1: [ True]\n",
      "This study explores using AI-based analytics to support design education by measuring students' use of space and scale in organizing their design work. Through a research artifact integrating a design analytics dashboard with design instances, instructors reflected on how these analytics could impact design education positively. The findings suggest that indexing design analytics to actual design work instances can help instructors assess and provide feedback effectively, especially in multiscale design contexts.\n",
      "http://arxiv.org/abs/2404.05427v1: [ True]\n",
      "Researchers have developed automated techniques using Large Language Models (LLMs) to assist in software development tasks such as issue summarization, bug reproduction, fault localization, and program repair. The proposed AutoCodeRover approach combines LLMs with sophisticated code search capabilities to autonomously improve programs by enhancing understanding of the issue's root cause and retrieving relevant context for modification or patching, showing increased efficacy in resolving real-life Github issues.\n",
      "http://arxiv.org/abs/2404.05569v1: [ True]\n",
      "Recent advancements in large language model agents focus on optimizing agent teams and implementing self-reflection to improve performance in complex tasks. A proposed framework called 360°REA utilizes a multi-agent approach with a 360° performance assessment method to enhance agent capabilities through experience accumulation, showing effectiveness in addressing complex tasks based on experimental results with various datasets.\n",
      "http://arxiv.org/abs/2404.05602v1: [ True]\n",
      "This research proposes an AI-powered cyber incident response system for cloud environments, using AI and ML for Network Traffic Classification and Malware Analysis, achieving high accuracy rates of 90% and 96% respectively with the Random Forest model. The study emphasizes the efficiency and scalability of AI/ML systems in cloud environments, utilizing container technology and cloud-based TPUs and GPUs for managing resource demands and ensuring a robust cyber incident response solution.\n",
      "http://arxiv.org/abs/2404.05602v2: [ True]\n",
      "This research proposes an AI-powered cyber incident response system for cloud environments using AI and ML techniques like the Random Forest model, achieving high accuracy rates. The study emphasizes the efficiency and scalability of AI/ML systems hosted in cloud environments, utilizing container technology and cloud-based TPUs and GPUs to manage resource demands.\n",
      "http://arxiv.org/abs/2404.05874v1: [ True]\n",
      "This study involved a two-week workshop where 13 youth aged 14-15 designed and audited ML-powered applications, leading to increased awareness of algorithmic biases and model design issues among participants. The findings suggest that involving youth in auditing algorithms can enhance their understanding of model functionality and inspire improvements in their own applications, contributing to research on child-computer interaction and learning.\n",
      "http://arxiv.org/abs/2404.05904v1: [ True]\n",
      "Large Language Models (LLMs) are powerful in NLP but can produce inaccurate outputs known as \"hallucinations.\" The Hallucinations Leaderboard quantitatively assesses different models' tendency to create hallucinations across various NLP tasks, helping researchers and practitioners select more reliable models for their needs.\n",
      "http://arxiv.org/abs/2404.05970v1: [ True]\n",
      "This paper explores retrieval-augmented methods for personalizing large language models, introducing optimization algorithms that improve the delivery of personal documents to enhance personalized generation. Through experimentation on various tasks, the study demonstrates significant enhancements in dataset performance by optimizing retrieval models for personalized generation tasks.\n",
      "http://arxiv.org/abs/2404.05990v1: [ True]\n",
      "Rapid advancements in Artificial Intelligence have led to increased power exerted over individuals through automated systems, impacting various aspects of society from government services to information dissemination and economic interactions. Scholars are advocating for a critical examination of these new power dynamics created by AI technologies to ensure proper authority and procedural legitimacy in their use.\n",
      "http://arxiv.org/abs/2404.06201v1: [ True]\n",
      "Large Language Models (LLMs) are crucial for improving software engineering tasks by enhancing code understanding, but their collaboration relies heavily on accessing high-quality data, which can be restricted due to commercial and privacy concerns, hindering progress in open-source AI-based software engineering projects. To address this challenge, a governance framework centered on federated learning (FL) is proposed to enable open-source AI models to leverage diverse organizational resources while ensuring data privacy and security, along with guidelines for developers on collaboration aspects such as data requirements, model architecture, updating strategies, and version control.\n",
      "http://arxiv.org/abs/2404.06209v1: [ True]\n",
      "This study explores how Large Language Models (LLMs) encounter issues of data contamination and memorization, particularly in handling tabular data, revealing that LLMs tend to memorize popular tabular datasets during training, leading to overfitting. The research shows that LLMs perform better on datasets they have seen during training, indicating a limitation in their in-context statistical learning abilities, yet they exhibit non-trivial performance on new datasets and data transformations.\n",
      "http://arxiv.org/abs/2404.06290v1: [ True]\n",
      "Large language models (LLMs) have shown significant success in natural language processing and beyond, with potential for artificial general intelligence in diverse optimization tasks. While LLMs may struggle in pure numerical tasks due to domain mismatch, they demonstrate promise in broader optimization contexts by leveraging heuristics from prompts to enhance performance.\n",
      "http://arxiv.org/abs/2404.06395v1: [ True]\n",
      "The text discusses the potential of Small Language Models (SLMs) as a cost-effective alternative to Large Language Models (LLMs), introducing MiniCPM models that perform well compared to larger LLMs. Through the use of a Warmup-Stable-Decay learning rate scheduler, the MiniCPM models demonstrate efficient data-model scaling and offer scalability for future LLM research.\n",
      "http://arxiv.org/abs/2404.06404v1: [ True]\n",
      "Large Language Models (LLMs) are powerful tools in research, offering benefits like cost-effectiveness and efficiency, but they also pose challenges such as prompt tuning, biases, and subjectivity that need to be addressed. This study explores the potential of LLMs through literature review and experimentation, highlighting successes, limitations, and strategies for mitigating challenges to integrate them into research ethically.\n",
      "http://arxiv.org/abs/2404.06411v1: [ True]\n",
      "Advances in Large Language Models have spurred the development of LLM agents for complex reasoning tasks, emphasizing the importance of benchmarking and evaluation for progress. The AgentQuest framework offers modular benchmarks and new evaluation metrics to track LLM agent progress effectively, highlighting common failure points and enhancing performance through refined architecture, aiming for community collaboration and extension.\n",
      "http://arxiv.org/abs/2404.06423v1: [ True]\n",
      "The article proposes using deep reinforcement learning to optimize a surveillance mission for an unmanned aerial vehicle with fuel or time constraints, ensuring it visits targets efficiently without running out of resources. Results show that the deep reinforcement learning algorithm outperformed traditional greedy heuristics in managing the vehicle's refueling or recharging needs while minimizing time between target visits.\n",
      "http://arxiv.org/abs/2404.06432v1: [ True]\n",
      "As AI-based decision aids become more common, the way uncertainty is presented in outcomes impacts users' trust in AI systems, especially in environments with inherent uncertainty like gig driving. A study with gig drivers using an AI scheduling tool found that trust and reliance on the tool were influenced by the accuracy of estimates, improved with ranged estimates, and were enhanced by increased prediction granularity and hedging language, emphasizing the importance of individualized trust-building approaches for AI systems.\n",
      "http://arxiv.org/abs/2404.06647v1: [ True]\n",
      "AI research has primarily focused on building larger deep learning models, leading to significant achievements in science and technology while hindering progress in explainability, ethics, and environmental efficiency. This shift in research approach towards benchmarking and measurable progress since the 1990s has streamlined advancements in AI but also limited exploration beyond scaling monoculture, raising concerns about the consolidation around external interests and lack of incentive for exploration in the field.\n",
      "http://arxiv.org/abs/2404.06910v1: [ True]\n",
      "Large language models have limitations in processing long contexts due to their quadratic inference cost, impacting real-world text processing applications like retrieval-augmented generation. A new superposition prompting methodology is proposed to enhance efficiency and accuracy by allowing parallel processing of input documents in pre-trained transformer-based LLMs without fine-tuning, resulting in significant improvements in compute time and accuracy for question-answering tasks.\n",
      "http://arxiv.org/abs/2404.07066v1: [ True]\n",
      "This paper investigates how large language models learn different concepts across their layers, with deeper layers handling more complex and abstract concepts. Through a probing technique, it shows that simpler tasks are efficiently classified in shallower layers, while deeper layers are crucial for discerning more complex tasks, impacting our understanding of model learning processes and representations.\n",
      "http://arxiv.org/abs/2404.07142v1: [ True]\n",
      "Software systems are crucial in modern society, but the diversity of software development teams often does not reflect the user base. To ensure inclusive work environments and usable software for diverse populations, efforts are needed to promote software developer diversity and inclusion, especially as technology like AI influences the landscape of software engineering.\n",
      "http://arxiv.org/abs/2404.07413v1: [ True]\n",
      "The JetMoE-8B is a cost-effective Large Language Model trained with minimal resources, showcasing impressive performance and outperforming other models, indicating that LLM training can be more affordable than previously believed. JetMoE-8B utilizes an efficient Sparsely-gated Mixture-of-Experts architecture, with sparsely activated layers reducing inference computation by about 70% compared to other models, and it promotes transparency, collaboration, and advancements in accessible and efficient LLM development.\n",
      "http://arxiv.org/abs/2404.07456v1: [ True]\n",
      "Large language models (LLMs) show promise as intelligent agents, but current research focuses on improving reasoning and decision-making rather than exploration and exploitation. A novel approach called Weak Exploration to Strong Exploitation (WESE) is proposed to address this gap by separating exploration and exploitation, utilizing a knowledge graph-based strategy to enhance task performance across various interactive benchmarks.\n",
      "http://arxiv.org/abs/2404.07461v1: [ True]\n",
      "A study examines how hallucination is understood in large language models by analyzing 103 research papers and conducting a survey with 171 NLP and AI experts, revealing a lack of consensus on the term and the need for clear definitions and frameworks in the field. The research highlights the importance of defining hallucination in NLP, emphasizing potential challenges and societal impacts, based on insights gathered from practitioners in the field.\n",
      "http://arxiv.org/abs/2404.07470v1: [ True]\n",
      "Continual learning is essential for expanding and refining language models, but current methods face challenges like reliance on experience replay and optimization constraints. The Scalable Language Model (SLM) introduced in this study, featuring Joint Adaptive Re-Parameterization (JARe) and Dynamic Task-related Knowledge Retrieval (DTKR), overcomes these limitations for practical applications, showcasing superior performance in various scenarios and task types with minimal forgetting.\n",
      "http://arxiv.org/abs/2404.07501v1: [ True]\n",
      "Research in business process modeling explores automated generation of process models from data, such as event logs and natural language texts, aiming to reduce costs and improve efficiency. This study investigates using data augmentation techniques from machine learning to enhance accuracy in extracting business process information from text, showing promising results in improving $F_1$ scores for mention and relation extraction. The findings suggest that data augmentation plays a crucial role in advancing machine learning methods for business process model generation from natural language text, offering insights through visualization and analysis of augmented textual data.\n",
      "http://arxiv.org/abs/2404.07546v1: [ True]\n",
      "In this study, researchers empirically analyze how In-context Learning (ICL) impacts the performance of large language models (LLMs) by breaking it down into label space, format, and discrimination dimensions. While demonstrations show marginal effect on improving discriminative knowledge, ICL significantly influences label space and format, acting as detailed instructions for LLMs to follow and enhancing model performance through retrieval of semantically similar examples.\n",
      "http://arxiv.org/abs/2404.07696v1: [ True]\n",
      "Utilizing few-shot classification and flatness-aware backbone training can enhance the adaptation of deep neural networks to new tasks with limited examples, proving crucial for achieving good generalization across various adaptation methods in real-world scenarios. Backbones trained with a focus on flatness, combined with vanilla fine-tuning, can serve as a simpler yet competitive baseline compared to current state-of-the-art approaches, showcasing the significance of backbone training in improving generalization for in- and cross-domain few-shot classification tasks.\n",
      "http://arxiv.org/abs/2404.07738v1: [ True]\n",
      "A ResearchAgent powered by a language model is proposed to streamline scientific research by automatically generating and refining research ideas, methods, and experiment designs through iterative processes based on existing literature. By connecting information from academic graphs and entity-centric knowledge stores, as well as incorporating peer review feedback, the ResearchAgent demonstrates effectiveness in producing innovative and valid research ideas across various disciplines.\n",
      "http://arxiv.org/abs/2404.07851v1: [ True]\n",
      "This work combines large language models (LLMs) with supervised machine translation (MT) systems by guiding LLMs to automatically post-edit MT with external feedback based on Multidimensional Quality Metric (MQM) annotations. Experiment results on Chinese-English, English-German, and English-Russian show that prompting LLMs to post-edit MT improves translation quality metrics like TER, BLEU, and COMET scores, while fine-tuning the LLMs enhances integration of feedback and improves translation quality further according to both automatic and human evaluation.\n",
      "http://arxiv.org/abs/2404.07926v1: [ True]\n",
      "The paper explores using Large Language Models (LLMs) as interactive tools to enhance collaboration between human coders and AI for annotating online risk data at scale, highlighting the need for improved methods supporting human-AI collaboration in data annotation tasks. It emphasizes the importance of two-way interactive discussions for nuanced and contextualized data annotation, suggesting LLMs as promising tools for future HCI research in reshaping data handling methods within the community.\n",
      "http://arxiv.org/abs/2404.08189v1: [ True]\n",
      "Generative AI often faces the issue of hallucinations, hindering user adoption, but the use of Retrieval Augmented Generation (RAG) can minimize this problem and enhance the quality of workflow outputs. By implementing RAG alongside a well-trained retriever encoder, the system effectively reduces hallucinations, improves generalization in diverse settings, and makes LLM-based deployments more efficient in terms of resource usage.\n",
      "http://arxiv.org/abs/2404.08361v2: [ True]\n",
      "Multi-domain learning is essential for improving feed recommendation systems by accurately capturing user interests across various scenarios. The Automatic Domain Feature Extraction and Personalized Integration (DFEI) framework proposed in the paper addresses the challenges of manually designing domain features and leveraging user behavior data to enhance recommendation accuracy, demonstrating superior performance compared to existing methods across 20 domains.\n",
      "http://arxiv.org/abs/2404.08509v1: [ True]\n",
      "Large language models are being used in various interactive AI applications, but serving their inference requests efficiently is challenging due to unpredictable execution times. To address this, a speculative shortest-job-first scheduler utilizing a light proxy model has been proposed, demonstrating significant reductions in job completion times and increased throughput compared to traditional first-come-first-serve schedulers.\n",
      "http://arxiv.org/abs/2404.08511v1: [ True]\n",
      "This study introduces a novel approach to cross-domain knowledge discovery by deploying specialized multi-AI agents that collaborate to provide comprehensive insights beyond single-domain expertise. Through comparative analysis, the research demonstrates the superior capability of domain-specific multi-AI agents in identifying and bridging knowledge gaps, highlighting the importance of collaborative AI in driving innovation and advancing cross-disciplinary research and applications.\n",
      "http://arxiv.org/abs/2404.08555v1: [ True]\n",
      "This study examines reinforcement learning from human feedback (RLHF) in large language models (LLMs), focusing on the role of reward models and training methods, while highlighting limitations such as incorrect generalization and model misspecification. The research aims to improve understanding of RLHF for LLMs and provides a reference for researchers and practitioners to address challenges and enhance current efforts in this area.\n",
      "http://arxiv.org/abs/2404.08570v1: [ True]\n",
      "The CRITICAL framework enhances autonomous vehicle training and testing by generating diverse scenarios targeting specific learning gaps through real-world traffic dynamics and optional Large Language Model analysis. By establishing a closed feedback loop between data generation and training, CRITICAL improves learning rates, system performance, and safety resilience, showing potential to enhance AV systems' robustness and accelerate their development.\n",
      "http://arxiv.org/abs/2404.08865v1: [ True]\n",
      "Thorough evaluations of Large Language Models (LLMs) are essential to understand their retrieval accuracy and contextual utilization, impacting their practical effectiveness. Research analyzing LLM recall performance through the needle-in-a-haystack method reveals that recall capability depends on prompt content, training biases, and can be enhanced through adjustments in model architecture or training strategies, guiding more effective LLM applications.\n",
      "http://arxiv.org/abs/2404.08940v1: [ True]\n",
      "A new approach called Super Retrieval-Augmented Generation (Super RAGs) is integrated into a top-tier Large Language Model (LLM) to improve performance by incorporating external knowledge sources with minimal structural changes, resulting in notable enhancements in accuracy, speed, and user satisfaction. The study showcases the effectiveness of Super RAGs in enhancing LLMs through a fine-tuned instruct model setup and cache tuning fork system, demonstrating significant improvements across various metrics over multiple evaluation epochs, indicating the potential for more advanced and dependable AI systems.\n",
      "http://arxiv.org/abs/2404.09022v1: [ True]\n",
      "This article reviews the increasing use of large models like ChatGPT in various industries and online platforms. It explores advanced fine-tuning methods such as task-adaptive, domain-adaptive, few-shot learning, knowledge distillation, multi-task learning, parameter-efficient, and dynamic fine-tuning for these models.\n",
      "http://arxiv.org/abs/2404.09248v1: [ True]\n",
      "The paper introduces KALM, a method that leverages large language models to train knowledgeable agents through imaginary rollouts, addressing the challenge of integrating textual data with numerical vectors in reinforcement learning. KALM enhances the agent's comprehension of environmental dynamics, allowing for successful completion of complex tasks and novel behaviors, surpassing baseline success rates by achieving a 46% success rate in executing tasks with unseen goals.\n",
      "http://arxiv.org/abs/2404.09296v1: [ True]\n",
      "Large language models (LLMs) are a significant focus in Artificial Intelligence, facing challenges in memory retention and domain-specific issues. To enhance their performance, researchers are exploring Retrieval-Augmented Generation (RAG) techniques and integrating LLMs with Knowledge Graphs (KGs) for factual context. The shift towards digital education is generating diverse educational data, prompting the automatic construction of Knowledge Graphs from various sources to improve question-answering tasks using LLMs.\n",
      "http://arxiv.org/abs/2404.09339v1: [ True]\n",
      "Large language models (LLMs) excel at language tasks but struggle with updating outdated information due to static knowledge stored in their parameters. Tool-based LLMs, despite needing adaptation to new environments, show promise for continual learning by relying less on parametric memory and more on applying pre-defined tools efficiently to solve tasks, as revealed through synthetic benchmarking and NLP task aggregation.\n",
      "http://arxiv.org/abs/2404.09384v1: [ True]\n",
      "Researchers have been exploring the use of prompts to maximize the potential of Large Language Models (LLMs) based on a review of 80 papers. The study aims to validate the concept of downstream tasks in prompt-based solutions and identify the types of tasks involved, ultimately creating a taxonomy to analyze engineering patterns in various Software Engineering problems such as testing, debugging, and program verification.\n",
      "http://arxiv.org/abs/2404.09521v1: [ True]\n",
      "This work explores zero-shot generalization in Reinforcement Learning by emphasizing the importance of contextual cues, such as gravity level, for robust adaptation to new environments. The proposed algorithm integrates context representation learning with policy learning, showing superior generalization in simulated domains, surpassing previous context-learning methods in zero-shot scenarios and advancing RL towards broad real-world task generalization.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_preds)):\n",
    "    if new_preds[i] == True:\n",
    "        paper_id = new_ids[i][\"paper_id\"]\n",
    "        summary = new_ids[i][\"concise_summary\"]\n",
    "        print(f\"{paper_id}: {new_preds[i]}\\n{summary}\")\n",
    "        formatted.append({\"id\": paper_id, \"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8735822f-444a-423a-a098-4a758e39b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'http://arxiv.org/abs/2404.04286v1', 'summary': 'Advancements in Large Language Models (LLMs) are leading to increased iterative interactions between models, with multi-round self-improving methods allowing for new example generation. By drawing parallels between LLM behavior and human cultural evolution using a Bayesian framework like Iterated Learning (IL), researchers aim to predict and guide the evolution of LLMs towards desired outcomes based on experimental verification.'}, {'id': 'http://arxiv.org/abs/2404.04298v1', 'summary': 'The study investigates whether Language Models (LLMs) can enhance their performance by refining previous outputs. Despite introducing a framework to evaluate generative and discriminative abilities, the experimental analysis of various LLMs suggests that they do not consistently excel in discrimination over initial generation, which could impact the advancement of self-improving AI systems.'}, {'id': 'http://arxiv.org/abs/2404.04361v1', 'summary': \"The study examines Large Language Models' (LLMs) effectiveness in predicting entity-specific sentiment from political news articles using zero-shot and few-shot strategies. The research highlights LLMs' superior performance over fine-tuned BERT models in capturing entity-specific sentiment, emphasizing the significance of suitable prompting strategies and model architectures in entity-centric sentiment analysis within the political news domain.\"}, {'id': 'http://arxiv.org/abs/2404.04442v1', 'summary': 'Large Language Models (LLMs) are advancing artificial intelligence by enabling autonomous agents with human-like text skills, poised to revolutionize industries like healthcare and customer service. Despite challenges like multimodality and value alignment, techniques such as prompting and reasoning are being explored to enhance these agents, with evaluation platforms like AgentBench and ToolLLM facilitating robust assessment in complex scenarios, paving the way for more capable autonomous agents that are anticipated to become integral in digital tasks from email responses to disease diagnosis.'}, {'id': 'http://arxiv.org/abs/2404.04570v1', 'summary': 'This text discusses the impact of large language models on AI system interaction patterns and highlights the lack of focus on human interaction with LLM in current literature reviews. The authors conducted a detailed review and mapping of 110 relevant publications, offering insights into human-LLM interaction patterns and identifying key challenges in this area.'}, {'id': 'http://arxiv.org/abs/2404.04750v2', 'summary': 'The advancements in artificial intelligence present both opportunities and challenges for society, with the potential to significantly impact humanity akin to the industrial revolution. The One Hundred Year Study on AI, founded by multidisciplinary experts, emphasizes the importance of understanding AI models, addressing societal impacts, and fostering collaboration among stakeholders to guide the development of AI responsibly for human benefit.'}, {'id': 'http://arxiv.org/abs/2404.04821v1', 'summary': 'AI is reshaping Software Engineering with a focus on Automated Software Evolution for intelligent applications, presenting complexity from data heterogeneity and changing contexts. A proposed conceptual framework emphasizes multimodality learning and offers a Selective Sequential Scope Model (3S) for categorizing research across SE phases, serving as a practical guide for practitioners venturing into this evolving domain.'}, {'id': 'http://arxiv.org/abs/2404.04834v1', 'summary': 'This paper discusses the integration of Large Language Models (LLMs) into autonomous agents to enhance cognitive abilities in addressing complex software engineering challenges through LLM-based Multi-Agent (LMA) systems, emphasizing benefits like collaborative problem-solving and scalability. It envisions the role of LMA systems in future software engineering practices, highlighting potential applications, emerging challenges, and research opportunities to guide future directions.'}, {'id': 'http://arxiv.org/abs/2404.04839v1', 'summary': 'DevOps and security integration into the workflow is crucial, with AI-driven security approaches showing promise in automating security tasks to ensure uninterrupted delivery speed. A comprehensive analysis of AI-driven security techniques applicable to DevOps has identified key challenges and opportunities for enhancing security, trust, and efficiency in software development processes.'}, {'id': 'http://arxiv.org/abs/2404.04949v1', 'summary': 'Large language models are being used in specialized fields, facing challenges with diverse data leading to conflicts during task transfer. The Adaptive Semantic Space Learning framework improves multi-expert model performance by reorganizing data distributions, as shown by the financial LLM \"SilverSight\" achieving high results with minimal data and strong generalization abilities.'}, {'id': 'http://arxiv.org/abs/2404.05417v1', 'summary': \"This study explores using AI-based analytics to support design education by measuring students' use of space and scale in organizing their design work. Through a research artifact integrating a design analytics dashboard with design instances, instructors reflected on how these analytics could impact design education positively. The findings suggest that indexing design analytics to actual design work instances can help instructors assess and provide feedback effectively, especially in multiscale design contexts.\"}, {'id': 'http://arxiv.org/abs/2404.05427v1', 'summary': \"Researchers have developed automated techniques using Large Language Models (LLMs) to assist in software development tasks such as issue summarization, bug reproduction, fault localization, and program repair. The proposed AutoCodeRover approach combines LLMs with sophisticated code search capabilities to autonomously improve programs by enhancing understanding of the issue's root cause and retrieving relevant context for modification or patching, showing increased efficacy in resolving real-life Github issues.\"}, {'id': 'http://arxiv.org/abs/2404.05569v1', 'summary': 'Recent advancements in large language model agents focus on optimizing agent teams and implementing self-reflection to improve performance in complex tasks. A proposed framework called 360°REA utilizes a multi-agent approach with a 360° performance assessment method to enhance agent capabilities through experience accumulation, showing effectiveness in addressing complex tasks based on experimental results with various datasets.'}, {'id': 'http://arxiv.org/abs/2404.05602v1', 'summary': 'This research proposes an AI-powered cyber incident response system for cloud environments, using AI and ML for Network Traffic Classification and Malware Analysis, achieving high accuracy rates of 90% and 96% respectively with the Random Forest model. The study emphasizes the efficiency and scalability of AI/ML systems in cloud environments, utilizing container technology and cloud-based TPUs and GPUs for managing resource demands and ensuring a robust cyber incident response solution.'}, {'id': 'http://arxiv.org/abs/2404.05602v2', 'summary': 'This research proposes an AI-powered cyber incident response system for cloud environments using AI and ML techniques like the Random Forest model, achieving high accuracy rates. The study emphasizes the efficiency and scalability of AI/ML systems hosted in cloud environments, utilizing container technology and cloud-based TPUs and GPUs to manage resource demands.'}, {'id': 'http://arxiv.org/abs/2404.05874v1', 'summary': 'This study involved a two-week workshop where 13 youth aged 14-15 designed and audited ML-powered applications, leading to increased awareness of algorithmic biases and model design issues among participants. The findings suggest that involving youth in auditing algorithms can enhance their understanding of model functionality and inspire improvements in their own applications, contributing to research on child-computer interaction and learning.'}, {'id': 'http://arxiv.org/abs/2404.05904v1', 'summary': 'Large Language Models (LLMs) are powerful in NLP but can produce inaccurate outputs known as \"hallucinations.\" The Hallucinations Leaderboard quantitatively assesses different models\\' tendency to create hallucinations across various NLP tasks, helping researchers and practitioners select more reliable models for their needs.'}, {'id': 'http://arxiv.org/abs/2404.05970v1', 'summary': 'This paper explores retrieval-augmented methods for personalizing large language models, introducing optimization algorithms that improve the delivery of personal documents to enhance personalized generation. Through experimentation on various tasks, the study demonstrates significant enhancements in dataset performance by optimizing retrieval models for personalized generation tasks.'}, {'id': 'http://arxiv.org/abs/2404.05990v1', 'summary': 'Rapid advancements in Artificial Intelligence have led to increased power exerted over individuals through automated systems, impacting various aspects of society from government services to information dissemination and economic interactions. Scholars are advocating for a critical examination of these new power dynamics created by AI technologies to ensure proper authority and procedural legitimacy in their use.'}, {'id': 'http://arxiv.org/abs/2404.06201v1', 'summary': 'Large Language Models (LLMs) are crucial for improving software engineering tasks by enhancing code understanding, but their collaboration relies heavily on accessing high-quality data, which can be restricted due to commercial and privacy concerns, hindering progress in open-source AI-based software engineering projects. To address this challenge, a governance framework centered on federated learning (FL) is proposed to enable open-source AI models to leverage diverse organizational resources while ensuring data privacy and security, along with guidelines for developers on collaboration aspects such as data requirements, model architecture, updating strategies, and version control.'}, {'id': 'http://arxiv.org/abs/2404.06209v1', 'summary': 'This study explores how Large Language Models (LLMs) encounter issues of data contamination and memorization, particularly in handling tabular data, revealing that LLMs tend to memorize popular tabular datasets during training, leading to overfitting. The research shows that LLMs perform better on datasets they have seen during training, indicating a limitation in their in-context statistical learning abilities, yet they exhibit non-trivial performance on new datasets and data transformations.'}, {'id': 'http://arxiv.org/abs/2404.06290v1', 'summary': 'Large language models (LLMs) have shown significant success in natural language processing and beyond, with potential for artificial general intelligence in diverse optimization tasks. While LLMs may struggle in pure numerical tasks due to domain mismatch, they demonstrate promise in broader optimization contexts by leveraging heuristics from prompts to enhance performance.'}, {'id': 'http://arxiv.org/abs/2404.06395v1', 'summary': 'The text discusses the potential of Small Language Models (SLMs) as a cost-effective alternative to Large Language Models (LLMs), introducing MiniCPM models that perform well compared to larger LLMs. Through the use of a Warmup-Stable-Decay learning rate scheduler, the MiniCPM models demonstrate efficient data-model scaling and offer scalability for future LLM research.'}, {'id': 'http://arxiv.org/abs/2404.06404v1', 'summary': 'Large Language Models (LLMs) are powerful tools in research, offering benefits like cost-effectiveness and efficiency, but they also pose challenges such as prompt tuning, biases, and subjectivity that need to be addressed. This study explores the potential of LLMs through literature review and experimentation, highlighting successes, limitations, and strategies for mitigating challenges to integrate them into research ethically.'}, {'id': 'http://arxiv.org/abs/2404.06411v1', 'summary': 'Advances in Large Language Models have spurred the development of LLM agents for complex reasoning tasks, emphasizing the importance of benchmarking and evaluation for progress. The AgentQuest framework offers modular benchmarks and new evaluation metrics to track LLM agent progress effectively, highlighting common failure points and enhancing performance through refined architecture, aiming for community collaboration and extension.'}, {'id': 'http://arxiv.org/abs/2404.06423v1', 'summary': \"The article proposes using deep reinforcement learning to optimize a surveillance mission for an unmanned aerial vehicle with fuel or time constraints, ensuring it visits targets efficiently without running out of resources. Results show that the deep reinforcement learning algorithm outperformed traditional greedy heuristics in managing the vehicle's refueling or recharging needs while minimizing time between target visits.\"}, {'id': 'http://arxiv.org/abs/2404.06432v1', 'summary': \"As AI-based decision aids become more common, the way uncertainty is presented in outcomes impacts users' trust in AI systems, especially in environments with inherent uncertainty like gig driving. A study with gig drivers using an AI scheduling tool found that trust and reliance on the tool were influenced by the accuracy of estimates, improved with ranged estimates, and were enhanced by increased prediction granularity and hedging language, emphasizing the importance of individualized trust-building approaches for AI systems.\"}, {'id': 'http://arxiv.org/abs/2404.06647v1', 'summary': 'AI research has primarily focused on building larger deep learning models, leading to significant achievements in science and technology while hindering progress in explainability, ethics, and environmental efficiency. This shift in research approach towards benchmarking and measurable progress since the 1990s has streamlined advancements in AI but also limited exploration beyond scaling monoculture, raising concerns about the consolidation around external interests and lack of incentive for exploration in the field.'}, {'id': 'http://arxiv.org/abs/2404.06910v1', 'summary': 'Large language models have limitations in processing long contexts due to their quadratic inference cost, impacting real-world text processing applications like retrieval-augmented generation. A new superposition prompting methodology is proposed to enhance efficiency and accuracy by allowing parallel processing of input documents in pre-trained transformer-based LLMs without fine-tuning, resulting in significant improvements in compute time and accuracy for question-answering tasks.'}, {'id': 'http://arxiv.org/abs/2404.07066v1', 'summary': 'This paper investigates how large language models learn different concepts across their layers, with deeper layers handling more complex and abstract concepts. Through a probing technique, it shows that simpler tasks are efficiently classified in shallower layers, while deeper layers are crucial for discerning more complex tasks, impacting our understanding of model learning processes and representations.'}, {'id': 'http://arxiv.org/abs/2404.07142v1', 'summary': 'Software systems are crucial in modern society, but the diversity of software development teams often does not reflect the user base. To ensure inclusive work environments and usable software for diverse populations, efforts are needed to promote software developer diversity and inclusion, especially as technology like AI influences the landscape of software engineering.'}, {'id': 'http://arxiv.org/abs/2404.07413v1', 'summary': 'The JetMoE-8B is a cost-effective Large Language Model trained with minimal resources, showcasing impressive performance and outperforming other models, indicating that LLM training can be more affordable than previously believed. JetMoE-8B utilizes an efficient Sparsely-gated Mixture-of-Experts architecture, with sparsely activated layers reducing inference computation by about 70% compared to other models, and it promotes transparency, collaboration, and advancements in accessible and efficient LLM development.'}, {'id': 'http://arxiv.org/abs/2404.07456v1', 'summary': 'Large language models (LLMs) show promise as intelligent agents, but current research focuses on improving reasoning and decision-making rather than exploration and exploitation. A novel approach called Weak Exploration to Strong Exploitation (WESE) is proposed to address this gap by separating exploration and exploitation, utilizing a knowledge graph-based strategy to enhance task performance across various interactive benchmarks.'}, {'id': 'http://arxiv.org/abs/2404.07461v1', 'summary': 'A study examines how hallucination is understood in large language models by analyzing 103 research papers and conducting a survey with 171 NLP and AI experts, revealing a lack of consensus on the term and the need for clear definitions and frameworks in the field. The research highlights the importance of defining hallucination in NLP, emphasizing potential challenges and societal impacts, based on insights gathered from practitioners in the field.'}, {'id': 'http://arxiv.org/abs/2404.07470v1', 'summary': 'Continual learning is essential for expanding and refining language models, but current methods face challenges like reliance on experience replay and optimization constraints. The Scalable Language Model (SLM) introduced in this study, featuring Joint Adaptive Re-Parameterization (JARe) and Dynamic Task-related Knowledge Retrieval (DTKR), overcomes these limitations for practical applications, showcasing superior performance in various scenarios and task types with minimal forgetting.'}, {'id': 'http://arxiv.org/abs/2404.07501v1', 'summary': 'Research in business process modeling explores automated generation of process models from data, such as event logs and natural language texts, aiming to reduce costs and improve efficiency. This study investigates using data augmentation techniques from machine learning to enhance accuracy in extracting business process information from text, showing promising results in improving $F_1$ scores for mention and relation extraction. The findings suggest that data augmentation plays a crucial role in advancing machine learning methods for business process model generation from natural language text, offering insights through visualization and analysis of augmented textual data.'}, {'id': 'http://arxiv.org/abs/2404.07546v1', 'summary': 'In this study, researchers empirically analyze how In-context Learning (ICL) impacts the performance of large language models (LLMs) by breaking it down into label space, format, and discrimination dimensions. While demonstrations show marginal effect on improving discriminative knowledge, ICL significantly influences label space and format, acting as detailed instructions for LLMs to follow and enhancing model performance through retrieval of semantically similar examples.'}, {'id': 'http://arxiv.org/abs/2404.07696v1', 'summary': 'Utilizing few-shot classification and flatness-aware backbone training can enhance the adaptation of deep neural networks to new tasks with limited examples, proving crucial for achieving good generalization across various adaptation methods in real-world scenarios. Backbones trained with a focus on flatness, combined with vanilla fine-tuning, can serve as a simpler yet competitive baseline compared to current state-of-the-art approaches, showcasing the significance of backbone training in improving generalization for in- and cross-domain few-shot classification tasks.'}, {'id': 'http://arxiv.org/abs/2404.07738v1', 'summary': 'A ResearchAgent powered by a language model is proposed to streamline scientific research by automatically generating and refining research ideas, methods, and experiment designs through iterative processes based on existing literature. By connecting information from academic graphs and entity-centric knowledge stores, as well as incorporating peer review feedback, the ResearchAgent demonstrates effectiveness in producing innovative and valid research ideas across various disciplines.'}, {'id': 'http://arxiv.org/abs/2404.07851v1', 'summary': 'This work combines large language models (LLMs) with supervised machine translation (MT) systems by guiding LLMs to automatically post-edit MT with external feedback based on Multidimensional Quality Metric (MQM) annotations. Experiment results on Chinese-English, English-German, and English-Russian show that prompting LLMs to post-edit MT improves translation quality metrics like TER, BLEU, and COMET scores, while fine-tuning the LLMs enhances integration of feedback and improves translation quality further according to both automatic and human evaluation.'}, {'id': 'http://arxiv.org/abs/2404.07926v1', 'summary': 'The paper explores using Large Language Models (LLMs) as interactive tools to enhance collaboration between human coders and AI for annotating online risk data at scale, highlighting the need for improved methods supporting human-AI collaboration in data annotation tasks. It emphasizes the importance of two-way interactive discussions for nuanced and contextualized data annotation, suggesting LLMs as promising tools for future HCI research in reshaping data handling methods within the community.'}, {'id': 'http://arxiv.org/abs/2404.08189v1', 'summary': 'Generative AI often faces the issue of hallucinations, hindering user adoption, but the use of Retrieval Augmented Generation (RAG) can minimize this problem and enhance the quality of workflow outputs. By implementing RAG alongside a well-trained retriever encoder, the system effectively reduces hallucinations, improves generalization in diverse settings, and makes LLM-based deployments more efficient in terms of resource usage.'}, {'id': 'http://arxiv.org/abs/2404.08361v2', 'summary': 'Multi-domain learning is essential for improving feed recommendation systems by accurately capturing user interests across various scenarios. The Automatic Domain Feature Extraction and Personalized Integration (DFEI) framework proposed in the paper addresses the challenges of manually designing domain features and leveraging user behavior data to enhance recommendation accuracy, demonstrating superior performance compared to existing methods across 20 domains.'}, {'id': 'http://arxiv.org/abs/2404.08509v1', 'summary': 'Large language models are being used in various interactive AI applications, but serving their inference requests efficiently is challenging due to unpredictable execution times. To address this, a speculative shortest-job-first scheduler utilizing a light proxy model has been proposed, demonstrating significant reductions in job completion times and increased throughput compared to traditional first-come-first-serve schedulers.'}, {'id': 'http://arxiv.org/abs/2404.08511v1', 'summary': 'This study introduces a novel approach to cross-domain knowledge discovery by deploying specialized multi-AI agents that collaborate to provide comprehensive insights beyond single-domain expertise. Through comparative analysis, the research demonstrates the superior capability of domain-specific multi-AI agents in identifying and bridging knowledge gaps, highlighting the importance of collaborative AI in driving innovation and advancing cross-disciplinary research and applications.'}, {'id': 'http://arxiv.org/abs/2404.08555v1', 'summary': 'This study examines reinforcement learning from human feedback (RLHF) in large language models (LLMs), focusing on the role of reward models and training methods, while highlighting limitations such as incorrect generalization and model misspecification. The research aims to improve understanding of RLHF for LLMs and provides a reference for researchers and practitioners to address challenges and enhance current efforts in this area.'}, {'id': 'http://arxiv.org/abs/2404.08570v1', 'summary': \"The CRITICAL framework enhances autonomous vehicle training and testing by generating diverse scenarios targeting specific learning gaps through real-world traffic dynamics and optional Large Language Model analysis. By establishing a closed feedback loop between data generation and training, CRITICAL improves learning rates, system performance, and safety resilience, showing potential to enhance AV systems' robustness and accelerate their development.\"}, {'id': 'http://arxiv.org/abs/2404.08865v1', 'summary': 'Thorough evaluations of Large Language Models (LLMs) are essential to understand their retrieval accuracy and contextual utilization, impacting their practical effectiveness. Research analyzing LLM recall performance through the needle-in-a-haystack method reveals that recall capability depends on prompt content, training biases, and can be enhanced through adjustments in model architecture or training strategies, guiding more effective LLM applications.'}, {'id': 'http://arxiv.org/abs/2404.08940v1', 'summary': 'A new approach called Super Retrieval-Augmented Generation (Super RAGs) is integrated into a top-tier Large Language Model (LLM) to improve performance by incorporating external knowledge sources with minimal structural changes, resulting in notable enhancements in accuracy, speed, and user satisfaction. The study showcases the effectiveness of Super RAGs in enhancing LLMs through a fine-tuned instruct model setup and cache tuning fork system, demonstrating significant improvements across various metrics over multiple evaluation epochs, indicating the potential for more advanced and dependable AI systems.'}, {'id': 'http://arxiv.org/abs/2404.09022v1', 'summary': 'This article reviews the increasing use of large models like ChatGPT in various industries and online platforms. It explores advanced fine-tuning methods such as task-adaptive, domain-adaptive, few-shot learning, knowledge distillation, multi-task learning, parameter-efficient, and dynamic fine-tuning for these models.'}, {'id': 'http://arxiv.org/abs/2404.09248v1', 'summary': \"The paper introduces KALM, a method that leverages large language models to train knowledgeable agents through imaginary rollouts, addressing the challenge of integrating textual data with numerical vectors in reinforcement learning. KALM enhances the agent's comprehension of environmental dynamics, allowing for successful completion of complex tasks and novel behaviors, surpassing baseline success rates by achieving a 46% success rate in executing tasks with unseen goals.\"}, {'id': 'http://arxiv.org/abs/2404.09296v1', 'summary': 'Large language models (LLMs) are a significant focus in Artificial Intelligence, facing challenges in memory retention and domain-specific issues. To enhance their performance, researchers are exploring Retrieval-Augmented Generation (RAG) techniques and integrating LLMs with Knowledge Graphs (KGs) for factual context. The shift towards digital education is generating diverse educational data, prompting the automatic construction of Knowledge Graphs from various sources to improve question-answering tasks using LLMs.'}, {'id': 'http://arxiv.org/abs/2404.09339v1', 'summary': 'Large language models (LLMs) excel at language tasks but struggle with updating outdated information due to static knowledge stored in their parameters. Tool-based LLMs, despite needing adaptation to new environments, show promise for continual learning by relying less on parametric memory and more on applying pre-defined tools efficiently to solve tasks, as revealed through synthetic benchmarking and NLP task aggregation.'}, {'id': 'http://arxiv.org/abs/2404.09384v1', 'summary': 'Researchers have been exploring the use of prompts to maximize the potential of Large Language Models (LLMs) based on a review of 80 papers. The study aims to validate the concept of downstream tasks in prompt-based solutions and identify the types of tasks involved, ultimately creating a taxonomy to analyze engineering patterns in various Software Engineering problems such as testing, debugging, and program verification.'}, {'id': 'http://arxiv.org/abs/2404.09521v1', 'summary': 'This work explores zero-shot generalization in Reinforcement Learning by emphasizing the importance of contextual cues, such as gravity level, for robust adaptation to new environments. The proposed algorithm integrates context representation learning with policy learning, showing superior generalization in simulated domains, surpassing previous context-learning methods in zero-shot scenarios and advancing RL towards broad real-world task generalization.'}]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(new_preds)):\n",
    "#     if new_preds[i] == True:\n",
    "#         print(f'{new_ids[i][\"paper_id\"]}')\n",
    "\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4835ea91-e2bd-485a-ae74-cf31c3fe11ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2404.04298v1: SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses\n",
      "http://arxiv.org/abs/2404.04361v1: Deciphering Political Entity Sentiment in News with Large Language\n",
      "  Models: Zero-Shot and Few-Shot Strategies\n",
      "http://arxiv.org/abs/2404.04540v1: The Case for Developing a Foundation Model for Planning-like Tasks from\n",
      "  Scratch\n",
      "http://arxiv.org/abs/2404.04570v1: A Map of Exploring Human Interaction patterns with LLM: Insights into\n",
      "  Collaboration and Creativity\n",
      "http://arxiv.org/abs/2404.04750v2: Now, Later, and Lasting: Ten Priorities for AI Research, Policy, and\n",
      "  Practice\n",
      "http://arxiv.org/abs/2404.04821v1: A Data-to-Product Multimodal Conceptual Framework to Achieve Automated\n",
      "  Software Evolution for Context-rich Intelligent Applications\n",
      "http://arxiv.org/abs/2404.04834v1: LLM-Based Multi-Agent Systems for Software Engineering: Vision and the\n",
      "  Road Ahead\n",
      "http://arxiv.org/abs/2404.04854v1: Contextual Chart Generation for Cyber Deception\n",
      "http://arxiv.org/abs/2404.04997v1: Adapting LLMs for Efficient Context Processing through Soft Prompt\n",
      "  Compression\n",
      "http://arxiv.org/abs/2404.05143v1: Plug and Play with Prompts: A Prompt Tuning Approach for Controlling\n",
      "  Text Generation\n",
      "http://arxiv.org/abs/2404.05213v1: Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor\n",
      "  When Adopting LLMs in HCI Research\n",
      "http://arxiv.org/abs/2404.05221v1: LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step\n",
      "  Reasoning with Large Language Models\n",
      "http://arxiv.org/abs/2404.05238v1: Allowing humans to interactively guide machines where to look does not\n",
      "  always improve a human-AI team's classification accuracy\n",
      "http://arxiv.org/abs/2404.05427v1: AutoCodeRover: Autonomous Program Improvement\n",
      "http://arxiv.org/abs/2404.05442v1: Unlocking Adaptive User Experience with Generative AI\n",
      "http://arxiv.org/abs/2404.05449v1: RoT: Enhancing Large Language Models with Reflection on Search Trees\n",
      "http://arxiv.org/abs/2404.05602v1: AI-Enabled System for Efficient and Effective Cyber Incident Detection\n",
      "  and Response in Cloud Environments\n",
      "http://arxiv.org/abs/2404.05602v2: AI-Enabled System for Efficient and Effective Cyber Incident Detection\n",
      "  and Response in Cloud Environments\n",
      "http://arxiv.org/abs/2404.05836v1: Unveiling Latent Topics in Robotic Process Automation -- an Approach\n",
      "  based on Latent Dirichlet Allocation Smart Review\n",
      "http://arxiv.org/abs/2404.05970v1: Optimization Methods for Personalizing Large Language Models through\n",
      "  Retrieval Augmentation\n",
      "http://arxiv.org/abs/2404.06063v1: All in One: An Empirical Study of GPT for Few-Shot Aspect-Based\n",
      "  Sentiment Anlaysis\n",
      "http://arxiv.org/abs/2404.06082v1: A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs\n",
      "http://arxiv.org/abs/2404.06162v1: Characterizing Multimodal Long-form Summarization: A Case Study on\n",
      "  Financial Reports\n",
      "http://arxiv.org/abs/2404.06201v1: Open-Source AI-based SE Tools: Opportunities and Challenges of\n",
      "  Collaborative Software Learning\n",
      "http://arxiv.org/abs/2404.06278v1: Dimensionality Reduction in Sentence Transformer Vector Databases with\n",
      "  Fast Fourier Transform\n",
      "http://arxiv.org/abs/2404.06311v1: DRE: Generating Recommendation Explanations by Aligning Large Language\n",
      "  Models at Data-level\n",
      "http://arxiv.org/abs/2404.06404v1: Apprentices to Research Assistants: Advancing Research with Large\n",
      "  Language Models\n",
      "http://arxiv.org/abs/2404.06411v1: AgentQuest: A Modular Benchmark Framework to Measure Progress and\n",
      "  Improve LLM Agents\n",
      "http://arxiv.org/abs/2404.06733v1: Incremental XAI: Memorable Understanding of AI with Incremental\n",
      "  Explanations\n",
      "http://arxiv.org/abs/2404.06777v1: Responsible Federated Learning in Smart Transportation: Outlooks and\n",
      "  Challenges\n",
      "http://arxiv.org/abs/2404.06910v1: Superposition Prompting: Improving and Accelerating Retrieval-Augmented\n",
      "  Generation\n",
      "http://arxiv.org/abs/2404.07143v1: Leave No Context Behind: Efficient Infinite Context Transformers with\n",
      "  Infini-attention\n",
      "http://arxiv.org/abs/2404.07413v1: JetMoE: Reaching Llama2 Performance with 0.1M Dollars\n",
      "http://arxiv.org/abs/2404.07461v1: \"Confidently Nonsensical?'': A Critical Survey on the Perspectives and\n",
      "  Challenges of 'Hallucinations' in NLP\n",
      "http://arxiv.org/abs/2404.07471v1: Structure-aware Fine-tuning for Code Pre-trained Models\n",
      "http://arxiv.org/abs/2404.07475v1: Laissez-Faire Harms: Algorithmic Biases in Generative Language Models\n",
      "http://arxiv.org/abs/2404.07499v1: Can Large Language Models Assess Serendipity in Recommender Systems?\n",
      "http://arxiv.org/abs/2404.07501v1: Leveraging Data Augmentation for Process Information Extraction\n",
      "http://arxiv.org/abs/2404.07738v1: ResearchAgent: Iterative Research Idea Generation over Scientific\n",
      "  Literature with Large Language Models\n",
      "http://arxiv.org/abs/2404.07851v1: Guiding Large Language Models to Post-Edit Machine Translation with\n",
      "  Error Annotations\n",
      "http://arxiv.org/abs/2404.07917v1: DesignQA: A Multimodal Benchmark for Evaluating Large Language Models'\n",
      "  Understanding of Engineering Documentation\n",
      "http://arxiv.org/abs/2404.07981v1: Manipulating Large Language Models to Increase Product Visibility\n",
      "http://arxiv.org/abs/2404.08189v1: Reducing hallucination in structured outputs via Retrieval-Augmented\n",
      "  Generation\n",
      "http://arxiv.org/abs/2404.08361v2: Large-Scale Multi-Domain Recommendation: an Automatic Domain Feature\n",
      "  Extraction and Personalized Integration Framework\n",
      "http://arxiv.org/abs/2404.08417v1: AdapterSwap: Continuous Training of LLMs with Data Removal and\n",
      "  Access-Control Guarantees\n",
      "http://arxiv.org/abs/2404.08480v1: Decoding AI: The inside story of data analysis in ChatGPT\n",
      "http://arxiv.org/abs/2404.08511v1: Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery\n",
      "http://arxiv.org/abs/2404.08721v1: Beyond One-Size-Fits-All: Adapting Counterfactual Explanations to User\n",
      "  Objectives\n",
      "http://arxiv.org/abs/2404.08940v1: Introducing Super RAGs in Mistral 8x7B-v1\n",
      "http://arxiv.org/abs/2404.09022v1: Navigating the Landscape of Large Language Models: A Comprehensive\n",
      "  Review and Analysis of Paradigms and Fine-Tuning Strategies\n",
      "http://arxiv.org/abs/2404.09554v1: Explainable Generative AI (GenXAI): A Survey, Conceptualization, and\n",
      "  Research Agenda\n"
     ]
    }
   ],
   "source": [
    "# Retrieve article titles\n",
    "\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, \"/home/woojay/P/ML/arxiver\")\n",
    "\n",
    "from arxiver.database import create_connection\n",
    "\n",
    "conn = create_connection(\"../data/arxiv_papers.db\")\n",
    "\n",
    "if conn is not None:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for i in range(len(new_preds)):\n",
    "        if new_preds[i] == True:\n",
    "            # Fetch the specific entry\n",
    "            cursor.execute(\n",
    "                \"SELECT paper_id, title, summary, concise_summary FROM papers WHERE paper_id = ?\",\n",
    "                (new_ids[i][\"paper_id\"],),\n",
    "            )\n",
    "            entry = cursor.fetchone()\n",
    "\n",
    "            if not entry:\n",
    "                conn.close()\n",
    "                raise HTTPException(status_code=404, detail=\"Paper not found\")\n",
    "\n",
    "            paper_id, title, summary, concise_summary = entry\n",
    "\n",
    "            print(f\"{paper_id}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110be7f-f5d9-48b9-abdd-af959e8dfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask openAI to pick the best articles:\n",
    "\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def choose_summaries(summaries, k):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert summarizer capable of distilling complex information into its essence and a skilled evaluator of cutting edge ideas. Your choices should be based on the most interesting, novel, and cutting edge ideas.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"From the following article summaries, pick the {k} most interesting, novel, and cutting edge ideas and return a json list with 'id' and 'summary' for each. The id should contain the article id. You may also include a 'reason' for each choice.: {summaries}\",\n",
    "                },\n",
    "            ],\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        response_content = (\n",
    "            response.choices[0]\n",
    "            .message.content.strip(\"`\")\n",
    "            .strip()\n",
    "            .removeprefix(\"json\\n\")\n",
    "        )\n",
    "\n",
    "        # Debugging\n",
    "        # print(\"Raw response content:\", response_content)\n",
    "\n",
    "        if response_content:\n",
    "            parsed_response = json.loads(response_content)\n",
    "            return parsed_response\n",
    "        else:\n",
    "            print(\"Response content is empty.\")\n",
    "            return []\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Failed to decode JSON:\", e)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4726e1-d9b5-4f7f-84b1-3b72aad43544",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = choose_summaries(formatted, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe38810-0cfe-4e4d-8a01-8acf67d95553",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67d15d-c069-4f35-8448-c60dff599d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "new_model = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            320,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train.shape[1],),\n",
    "            kernel_regularizer=l2(0.001),\n",
    "        ),\n",
    "        Dropout(0.0),\n",
    "        BatchNormalization(),\n",
    "        Dense(224, activation=\"relu\", kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b976dd0-7b4d-4891-bac7-d8cdc925cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa1df4-69c8-439f-b489-ee0427e0a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predicted articles\n",
    "\n",
    "new_x = []\n",
    "formatted = []\n",
    "for id in new_ids:\n",
    "    new_x.append(get_embedding(id[\"paper_id\"]))\n",
    "\n",
    "new_preds = model.predict(new_x) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711747c-8c62-492c-b450-14e4dd6f1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_preds)):\n",
    "    if new_preds[i] == True:\n",
    "        paper_id = new_ids[i][\"paper_id\"]\n",
    "        summary = new_ids[i][\"concise_summary\"]\n",
    "        print(f\"{paper_id}: {new_preds[i]}\\n{summary}\")\n",
    "        formatted.append({\"id\": paper_id, \"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e31e9-9ae4-404a-b5ae-8ada5fecc091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
