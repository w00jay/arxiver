{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c040e712-d339-43cb-bdff-03680a5a2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from chromadb.utils import embedding_functions\n",
    "from keras_tuner import RandomSearch\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1f98c9b-ad5b-43e0-b7ab-4e4a66276d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CUTOFF = 2711  # 2424 2155 1880 1572 1016 817 502 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34115fc5-9f10-4615-8fc6-54f1206885da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_interested_entry(database_path):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQL query to find the last occurrence of \"interested\" = 1\n",
    "    query = \"\"\"\n",
    "    SELECT paper_id FROM papers \n",
    "    WHERE interested = 1 \n",
    "    ORDER BY paper_id ASC \n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        last_interested = cursor.fetchone()\n",
    "        if last_interested:\n",
    "            print(\"Last interested entry:\", last_interested)\n",
    "            return {\"paper_id\": last_interested[0]}\n",
    "        else:\n",
    "            print(\"No interested entries found.\")\n",
    "            return nil\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe783ae0-0c05-4ec7-8e9b-33d972deab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last interested entry: ('http://arxiv.org/abs/2403.17287v1',)\n",
      "{\n",
      "    \"paper_id\": \"http://arxiv.org/abs/2403.17287v1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "db = \"../data/arxiv_papers.db\"\n",
    "last_interested = find_last_interested_entry(db)\n",
    "print(json.dumps(last_interested, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca3218c5-a977-4691-b11f-760b7e620bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(database_path):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.row_factory = sqlite3.Row\n",
    "\n",
    "    # SQL query to find the last occurrence of \"interested\" = 1\n",
    "    query = \"\"\"\n",
    "    SELECT paper_id, concise_summary, interested FROM papers \n",
    "    ORDER BY paper_id ASC \n",
    "    LIMIT 2711;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        articles = cursor.fetchall()\n",
    "        if articles:\n",
    "            print(f\"Got {len(articles)}.\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(\"No interested entries found.\")\n",
    "            return nil\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "115b2d07-576a-4260-9165-4d2772b009f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(paper_id, vdb_path=\"../data/arxiv_embeddings.chroma\"):\n",
    "    vdb = chromadb.PersistentClient(vdb_path)\n",
    "    sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    embedding_func = sentence_transformer_ef\n",
    "    vectors = vdb.get_or_create_collection(\n",
    "        name=\"arxiver\", embedding_function=embedding_func\n",
    "    )\n",
    "\n",
    "    res = vectors.get(ids=[paper_id], limit=1, include=[\"embeddings\"])\n",
    "    # print(res)\n",
    "    # print(f'{res[\"ids\"][0]} {res[\"embeddings\"][0]}')\n",
    "    return res[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc22c976-1746-4844-8dfd-d194e6c26da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_embedding(articles[0][\"paper_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3abb7b26-c040-411e-800c-80254d8680c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2711.\n",
      "(2711, 384) (2711,)\n"
     ]
    }
   ],
   "source": [
    "articles = get_data(db)\n",
    "X_article = []\n",
    "y_article = []\n",
    "for article in articles:\n",
    "    # print(\n",
    "    #     f'{article[\"paper_id\"]}, {article[\"interested\"]}\\n{article[\"concise_summary\"]}'\n",
    "    # )\n",
    "    X_article.append(get_embedding(article[\"paper_id\"]))\n",
    "    y_article.append(article[\"interested\"])\n",
    "\n",
    "# print(X_article[:3])\n",
    "# print(y_article[:3])\n",
    "\n",
    "X = np.array(X_article)\n",
    "y = np.array(y_article)\n",
    "\n",
    "# print(X[:1])\n",
    "# print(y[:1])\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05805479-ad97-46da-907d-90957faac493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e62a7967-49b5-48ee-9f5e-4b03a83bf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4624ccb4-0a03-48fc-b910-557dfb5d9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(384, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "532f9ae5-0a9e-429b-a1f8-d442f3582efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 4s 35ms/step - loss: 0.3002 - accuracy: 0.9319 - val_loss: 0.2068 - val_accuracy: 0.9447\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.1771 - accuracy: 0.9493 - val_loss: 0.2027 - val_accuracy: 0.9447\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.1568 - accuracy: 0.9493 - val_loss: 0.2069 - val_accuracy: 0.9447\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.1379 - accuracy: 0.9493 - val_loss: 0.2199 - val_accuracy: 0.9447\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.1234 - accuracy: 0.9493 - val_loss: 0.2456 - val_accuracy: 0.9447\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.1078 - accuracy: 0.9498 - val_loss: 0.2468 - val_accuracy: 0.9447\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 2s 28ms/step - loss: 0.0909 - accuracy: 0.9539 - val_loss: 0.2717 - val_accuracy: 0.9516\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.0706 - accuracy: 0.9671 - val_loss: 0.2909 - val_accuracy: 0.9470\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.0526 - accuracy: 0.9821 - val_loss: 0.3357 - val_accuracy: 0.9493\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.0313 - accuracy: 0.9913 - val_loss: 0.3748 - val_accuracy: 0.9493\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.0177 - accuracy: 0.9977 - val_loss: 0.4045 - val_accuracy: 0.9309\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.0109 - accuracy: 0.9994 - val_loss: 0.4524 - val_accuracy: 0.9424\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.4797 - val_accuracy: 0.9424\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 1s 23ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.4947 - val_accuracy: 0.9332\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5294 - val_accuracy: 0.9378\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5375 - val_accuracy: 0.9401\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.9447\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 8.1270e-04 - accuracy: 1.0000 - val_loss: 0.5609 - val_accuracy: 0.9401\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 6.6957e-04 - accuracy: 1.0000 - val_loss: 0.5768 - val_accuracy: 0.9447\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 6.1391e-04 - accuracy: 1.0000 - val_loss: 0.5820 - val_accuracy: 0.9424\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 5.3786e-04 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.9424\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 4.6463e-04 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 0.9447\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 3.3669e-04 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 0.9424\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 3.4494e-04 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 0.9447\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 2.6005e-04 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 0.9447\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 3.0389e-04 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 0.9447\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 2.1521e-04 - accuracy: 1.0000 - val_loss: 0.6321 - val_accuracy: 0.9447\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 2.0308e-04 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 0.9447\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 2.3366e-04 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.9470\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 1.9396e-04 - accuracy: 1.0000 - val_loss: 0.6452 - val_accuracy: 0.9447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f40848370d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "473d8456-003d-48ca-aec8-fa6c8f99bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import datetime\n",
    "\n",
    "formatted_time = datetime.datetime.now().strftime(f\"%Y%m%d_%H%M\")\n",
    "model.save(f\"model-{formatted_time}-{TRAIN_CUTOFF}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a43cedba-e5ff-413e-b2e1-89c2dfd1e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       514\n",
      "           1       0.17      0.07      0.10        29\n",
      "\n",
      "    accuracy                           0.93       543\n",
      "   macro avg       0.56      0.52      0.53       543\n",
      "weighted avg       0.91      0.93      0.92       543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "predictions = model.predict(X_test) > 0.5\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53ab900e-d34b-4920-adf1-fb1c32f99f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data(database_path):\n",
    "    # Create a database connection\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.row_factory = sqlite3.Row\n",
    "\n",
    "    # SQL query to find the last occurrence of \"interested\" = 1\n",
    "    query = \"\"\"\n",
    "    SELECT paper_id, concise_summary FROM papers\n",
    "    ORDER BY paper_id ASC\n",
    "    LIMIT 2000 OFFSET 2711;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        articles = cursor.fetchall()\n",
    "        if articles:\n",
    "            print(f\"Got {len(articles)}.\")\n",
    "            return articles\n",
    "        else:\n",
    "            print(\"No interested entries found.\")\n",
    "            return nil\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f93e5fb3-87a4-40ee-be94-0eda3ae02a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/abs/2404.03478v1'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ids = get_new_data(db)\n",
    "\n",
    "new_ids[0][\"paper_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6dbb32a-31dc-44f0-b399-95c974a909f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted articles\n",
    "\n",
    "new_x = []\n",
    "formatted = []\n",
    "for id in new_ids:\n",
    "    new_x.append(get_embedding(id[\"paper_id\"]))\n",
    "\n",
    "new_preds = model.predict(new_x) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0e3d6c35-587d-44d0-953e-809e78bf047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2404.03502v1: [ True]\n",
      "Artificial intelligence can boost productivity and generate insights, but its widespread use may have unintended consequences, such as harming public understanding. The reliance on recursive AI systems could lead to \"knowledge collapse,\" impacting innovation and human culture, as demonstrated by a model where a discount on AI-generated content leads to public beliefs further from the truth. Further research is needed to address and mitigate these outcomes.\n",
      "http://arxiv.org/abs/2404.03558v1: [ True]\n",
      "Large language models have demonstrated the ability to perform new tasks with limited examples through in-context learning, with multi-task learning offering promising transfer learning potential. This research explores combining multi-task learning with in-context learning to create models that efficiently learn tasks and are robust to out-of-distribution examples, proposing curriculum learning strategies for higher data efficiency and stable convergence. The experiments show that in-context learning models can effectively tackle difficult tasks by training on progressively harder tasks while mixing in prior tasks, enhancing generalization and robustness.\n",
      "http://arxiv.org/abs/2404.03631v1: [ True]\n",
      "The text discusses the use of Task Vectors (TV) for unconditionally erasing concepts from a text-to-image model, showing that TV-based erasure is more robust to unexpected user inputs but can impact the model's core performance. The proposed Diverse Inversion method helps estimate the required edit strength of the TV edit by finding diverse word embeddings that induce the generation of the target concept, allowing for selective erasure of model weights to enhance erasure capabilities while preserving core functionality.\n",
      "http://arxiv.org/abs/2404.03880v1: [ True]\n",
      "Advancements in Machine Learning have led to various methods for analyzing unstructured data such as images and text, enabling meaningful information extraction and storage in relational databases for SQL queries. A novel framework called SSQL integrates semantic queries with SQL statements, optimizing queries by combining semantic predicates with structured table predicates and incorporating human-in-the-loop feedback for improved results.\n",
      "http://arxiv.org/abs/2404.03995v1: [ True]\n",
      "Recent advancements in AI technology have prompted companies to consider integrating AI into software systems for sustainability purposes, despite potential negative impacts. The study synthesized trade-offs related to sustainability to provide practitioners with transparent insights on the benefits and costs of AI integration, identifying key themes like energy management, employee wellbeing, deployment issues, ethics & society, and environmental issues.\n",
      "http://arxiv.org/abs/2404.04204v1: [ True]\n",
      "This perspective paper highlights barriers to social skill training and proposes leveraging large language models through an AI Mentor framework for more accessible and realistic practice with tailored feedback. The aim is to enhance social skill development across various fields and promote workforce development and social equality through cross-disciplinary innovation.\n",
      "http://arxiv.org/abs/2404.04237v1: [ True]\n",
      "Large language models (LLMs) have shown remarkable capabilities in surpassing human performance on diverse tasks but can struggle with simple tasks, emphasizing the need for improved evaluation methods. A study on compositional and conditional reasoning in LLMs, using a flight booking benchmark called GroundCocoa, revealed varying performance levels among state-of-the-art models, with the top-performing model achieving a maximum accuracy of 67%.\n",
      "http://arxiv.org/abs/2404.04286v1: [ True]\n",
      "Advancements in Large Language Models (LLMs) are leading to increased iterative interactions between models, with multi-round self-improving methods allowing for new example generation. By drawing parallels between LLM behavior and human cultural evolution using a Bayesian framework like Iterated Learning (IL), researchers aim to predict and guide the evolution of LLMs towards desired outcomes based on experimental verification.\n",
      "http://arxiv.org/abs/2404.04289v1: [ True]\n",
      "Researchers conducted a qualitative study on designing agents for negotiating tasks, finding that successful performance requires alignment over six dimensions, including knowledge schema, autonomy, training, reputation, ethics, and human engagement. These findings contribute to understanding the importance of aligning values and safety in Human-AI interactions, leading to proposed design directions for future Human-Agent collaborations.\n",
      "http://arxiv.org/abs/2404.04500v1: [ True]\n",
      "The text discusses the conflict between business secrecy and the societal need for transparency in algorithms. A protocol called ZkAudit is proposed to address this issue by allowing model providers to keep certain aspects secret while enabling trustless audits of model and data properties through cryptographic commitments and zero-knowledge proofs.\n",
      "http://arxiv.org/abs/2404.04540v1: [ True]\n",
      "Foundation models (FMs) have transformed various computing domains such as Automated Planning and Scheduling (APS) by aiding in tasks like plan generation, translation, and optimization. This paper emphasizes the importance of developing a comprehensive FM specifically for planning-like (PL) tasks like business processes and workflows to enhance problem-solving capabilities similar to how large language models (LLMs) are benefiting APS.\n",
      "http://arxiv.org/abs/2404.04566v1: [ True]\n",
      "Large Language Models (LLMs) have advanced software engineering tasks, leading to the emergence of the Large Language Models for Software Engineering (LLM4SE) field. This position paper advocates for prioritizing efficient and environmentally friendly LLM4SE solutions, proposing a roadmap for future research to achieve this goal and revolutionize the software engineering landscape.\n",
      "http://arxiv.org/abs/2404.04570v1: [ True]\n",
      "This text discusses the impact of large language models on AI system interaction patterns and highlights the lack of focus on human interaction with LLM in current literature reviews. The authors conducted a detailed review and mapping of 110 relevant publications, offering insights into human-LLM interaction patterns and identifying key challenges in this area.\n",
      "http://arxiv.org/abs/2404.04572v1: [ True]\n",
      "Sustainability is a key concern in Machine Learning enabled Systems, where ML models often struggle in production due to uncertainties like data variations and model instabilities. Machine Learning Operations (MLOps) aims to enhance adaptability in these systems, but faces challenges such as environmental impact and technical maintenance, which can be addressed through self-adaptive principles like the MAPE-K loop to improve sustainability and address uncertainties.\n",
      "http://arxiv.org/abs/2404.04750v2: [ True]\n",
      "The advancements in artificial intelligence present both opportunities and challenges for society, with the potential to significantly impact humanity akin to the industrial revolution. The One Hundred Year Study on AI, founded by multidisciplinary experts, emphasizes the importance of understanding AI models, addressing societal impacts, and fostering collaboration among stakeholders to guide the development of AI responsibly for human benefit.\n",
      "http://arxiv.org/abs/2404.04821v1: [ True]\n",
      "AI is reshaping Software Engineering with a focus on Automated Software Evolution for intelligent applications, presenting complexity from data heterogeneity and changing contexts. A proposed conceptual framework emphasizes multimodality learning and offers a Selective Sequential Scope Model (3S) for categorizing research across SE phases, serving as a practical guide for practitioners venturing into this evolving domain.\n",
      "http://arxiv.org/abs/2404.04834v1: [ True]\n",
      "This paper discusses the integration of Large Language Models (LLMs) into autonomous agents to enhance cognitive abilities in addressing complex software engineering challenges through LLM-based Multi-Agent (LMA) systems, emphasizing benefits like collaborative problem-solving and scalability. It envisions the role of LMA systems in future software engineering practices, highlighting potential applications, emerging challenges, and research opportunities to guide future directions.\n",
      "http://arxiv.org/abs/2404.04839v1: [ True]\n",
      "DevOps and security integration into the workflow is crucial, with AI-driven security approaches showing promise in automating security tasks to ensure uninterrupted delivery speed. A comprehensive analysis of AI-driven security techniques applicable to DevOps has identified key challenges and opportunities for enhancing security, trust, and efficiency in software development processes.\n",
      "http://arxiv.org/abs/2404.04949v1: [ True]\n",
      "Large language models are being used in specialized fields, facing challenges with diverse data leading to conflicts during task transfer. The Adaptive Semantic Space Learning framework improves multi-expert model performance by reorganizing data distributions, as shown by the financial LLM \"SilverSight\" achieving high results with minimal data and strong generalization abilities.\n",
      "http://arxiv.org/abs/2404.05228v1: [ True]\n",
      "This study focused on developing and evaluating an AI system to educate individuals on making unbiased decisions by using fairness-aware machine learning. Participants who received AI guidance for fair decision-making showed a willingness to reassess their views on fairness, reflect on biases, and adjust decision-making criteria, highlighting the potential of AI systems in promoting unbiased decision-making in humans.\n",
      "http://arxiv.org/abs/2404.05365v1: [ True]\n",
      "The paper discusses the marginalization of indigenous language communities in the face of technological advancements, emphasizing the importance of inclusive NLP innovations that respect their cultural richness. It highlights the NLP progress of indigenous Latin American languages and the challenges and innovations needed for their preservation and development, aiming to bridge the gap between these communities and researchers.\n",
      "http://arxiv.org/abs/2404.05442v1: [ True]\n",
      "This paper explores using ChatGPT, a generative AI tool, to develop user personas and adaptive interfaces as an alternative to traditional manual processes, showing promising results in improving the design process efficiency. By leveraging the power of Large Language Models, there is potential to streamline the time, effort, and cost associated with user research for user-centered applications.\n",
      "http://arxiv.org/abs/2404.05449v1: [ True]\n",
      "A framework called Reflection on search Trees (RoT) is introduced to enhance the performance of large language models (LLMs) in reasoning and planning tasks by summarizing guidelines from previous search experiences to prevent repeated mistakes. RoT significantly improves LLM performance with tree-search-based prompting methods like BFS and MCTS, and can also benefit non-tree-search-based methods by providing task-specific knowledge derived from search experiences.\n",
      "http://arxiv.org/abs/2404.05569v1: [ True]\n",
      "Recent advancements in large language model agents focus on optimizing agent teams and implementing self-reflection to improve performance in complex tasks. A proposed framework called 360°REA utilizes a multi-agent approach with a 360° performance assessment method to enhance agent capabilities through experience accumulation, showing effectiveness in addressing complex tasks based on experimental results with various datasets.\n",
      "http://arxiv.org/abs/2404.05602v1: [ True]\n",
      "This research proposes an AI-powered cyber incident response system for cloud environments, using AI and ML for Network Traffic Classification and Malware Analysis, achieving high accuracy rates of 90% and 96% respectively with the Random Forest model. The study emphasizes the efficiency and scalability of AI/ML systems in cloud environments, utilizing container technology and cloud-based TPUs and GPUs for managing resource demands and ensuring a robust cyber incident response solution.\n",
      "http://arxiv.org/abs/2404.05779v1: [ True]\n",
      "Data quality is vital for effective AI models, with poor data leading to inaccuracies and potential risks. Current research aims to establish standardized metrics for evaluating data readiness to improve the quality and accuracy of AI training and inference.\n",
      "http://arxiv.org/abs/2404.05874v1: [ True]\n",
      "This study involved a two-week workshop where 13 youth aged 14-15 designed and audited ML-powered applications, leading to increased awareness of algorithmic biases and model design issues among participants. The findings suggest that involving youth in auditing algorithms can enhance their understanding of model functionality and inspire improvements in their own applications, contributing to research on child-computer interaction and learning.\n",
      "http://arxiv.org/abs/2404.05904v1: [ True]\n",
      "Large Language Models (LLMs) are powerful in NLP but can produce inaccurate outputs known as \"hallucinations.\" The Hallucinations Leaderboard quantitatively assesses different models' tendency to create hallucinations across various NLP tasks, helping researchers and practitioners select more reliable models for their needs.\n",
      "http://arxiv.org/abs/2404.05990v1: [ True]\n",
      "Rapid advancements in Artificial Intelligence have led to increased power exerted over individuals through automated systems, impacting various aspects of society from government services to information dissemination and economic interactions. Scholars are advocating for a critical examination of these new power dynamics created by AI technologies to ensure proper authority and procedural legitimacy in their use.\n",
      "http://arxiv.org/abs/2404.06201v1: [ True]\n",
      "Large Language Models (LLMs) are crucial for improving software engineering tasks by enhancing code understanding, but their collaboration relies heavily on accessing high-quality data, which can be restricted due to commercial and privacy concerns, hindering progress in open-source AI-based software engineering projects. To address this challenge, a governance framework centered on federated learning (FL) is proposed to enable open-source AI models to leverage diverse organizational resources while ensuring data privacy and security, along with guidelines for developers on collaboration aspects such as data requirements, model architecture, updating strategies, and version control.\n",
      "http://arxiv.org/abs/2404.06404v1: [ True]\n",
      "Large Language Models (LLMs) are powerful tools in research, offering benefits like cost-effectiveness and efficiency, but they also pose challenges such as prompt tuning, biases, and subjectivity that need to be addressed. This study explores the potential of LLMs through literature review and experimentation, highlighting successes, limitations, and strategies for mitigating challenges to integrate them into research ethically.\n",
      "http://arxiv.org/abs/2404.06411v1: [ True]\n",
      "Advances in Large Language Models have spurred the development of LLM agents for complex reasoning tasks, emphasizing the importance of benchmarking and evaluation for progress. The AgentQuest framework offers modular benchmarks and new evaluation metrics to track LLM agent progress effectively, highlighting common failure points and enhancing performance through refined architecture, aiming for community collaboration and extension.\n",
      "http://arxiv.org/abs/2404.06664v1: [ True]\n",
      "Current methods for evaluating the multicultural knowledge of large language models struggle to capture the complexity and diversity of cultural norms, potentially perpetuating biases in their benchmarks. An interactive system called CulturalTeaming leverages human-AI collaboration to create challenging evaluation datasets that expose shortcomings in modern LLMs' multicultural proficiency, with accuracy ranging from 37.7% to 72.2% across different LLM families.\n",
      "http://arxiv.org/abs/2404.06777v1: [ True]\n",
      "Integrating AI and federated learning in smart transportation raises concerns about responsible use, crucial for system stability and sustainability. Research on the responsible application of AI and FL in this area is limited, with a need for more thorough exploration and solutions to challenges in developing and implementing responsible FL.\n",
      "http://arxiv.org/abs/2404.06948v1: [ True]\n",
      "The paper introduces a winning solution for the SemEval-2024 Task 6 competition, utilizing a meta-regressor framework of large language models (LLMs) to evaluate and integrate models effectively, achieving top scores on the leader board. By harnessing uncertainty signals from a variety of LLMs, the approach enhances the detection of hallucinations with greater robustness.\n",
      "http://arxiv.org/abs/2404.06954v1: [ True]\n",
      "Dynamic computation methods for Large Language Models have sped up processing by skipping layers through heuristics or predictors, but existing decoding processes assign different computational budgets to samples, leading to instability. The proposed Unified Layer Skipping strategy selects the number of layers to skip based on a speedup ratio, balancing computation skipping to improve inference performance and model throughput for tasks like machine translation and text summarization.\n",
      "http://arxiv.org/abs/2404.07066v1: [ True]\n",
      "This paper investigates how large language models learn different concepts across their layers, with deeper layers handling more complex and abstract concepts. Through a probing technique, it shows that simpler tasks are efficiently classified in shallower layers, while deeper layers are crucial for discerning more complex tasks, impacting our understanding of model learning processes and representations.\n",
      "http://arxiv.org/abs/2404.07142v1: [ True]\n",
      "Software systems are crucial in modern society, but the diversity of software development teams often does not reflect the user base. To ensure inclusive work environments and usable software for diverse populations, efforts are needed to promote software developer diversity and inclusion, especially as technology like AI influences the landscape of software engineering.\n",
      "http://arxiv.org/abs/2404.07413v1: [ True]\n",
      "The JetMoE-8B is a cost-effective Large Language Model trained with minimal resources, showcasing impressive performance and outperforming other models, indicating that LLM training can be more affordable than previously believed. JetMoE-8B utilizes an efficient Sparsely-gated Mixture-of-Experts architecture, with sparsely activated layers reducing inference computation by about 70% compared to other models, and it promotes transparency, collaboration, and advancements in accessible and efficient LLM development.\n",
      "http://arxiv.org/abs/2404.07461v1: [ True]\n",
      "A study examines how hallucination is understood in large language models by analyzing 103 research papers and conducting a survey with 171 NLP and AI experts, revealing a lack of consensus on the term and the need for clear definitions and frameworks in the field. The research highlights the importance of defining hallucination in NLP, emphasizing potential challenges and societal impacts, based on insights gathered from practitioners in the field.\n",
      "http://arxiv.org/abs/2404.07501v1: [ True]\n",
      "Research in business process modeling explores automated generation of process models from data, such as event logs and natural language texts, aiming to reduce costs and improve efficiency. This study investigates using data augmentation techniques from machine learning to enhance accuracy in extracting business process information from text, showing promising results in improving $F_1$ scores for mention and relation extraction. The findings suggest that data augmentation plays a crucial role in advancing machine learning methods for business process model generation from natural language text, offering insights through visualization and analysis of augmented textual data.\n",
      "http://arxiv.org/abs/2404.07678v1: [ True]\n",
      "To thrive, organizations must prioritize innovation and address ethical and sustainability concerns, especially in today's fast-paced environment. This involves integrating ethical and sustainability considerations, particularly in the adoption of emerging technologies like artificial intelligence, to foster innovation cultures in both startups and established enterprises using approaches such as systems thinking.\n",
      "http://arxiv.org/abs/2404.07738v1: [ True]\n",
      "A ResearchAgent powered by a language model is proposed to streamline scientific research by automatically generating and refining research ideas, methods, and experiment designs through iterative processes based on existing literature. By connecting information from academic graphs and entity-centric knowledge stores, as well as incorporating peer review feedback, the ResearchAgent demonstrates effectiveness in producing innovative and valid research ideas across various disciplines.\n",
      "http://arxiv.org/abs/2404.08335v1: [ True]\n",
      "Research has shown that tokenization is essential for creating high-performing language models, as demonstrated by transformers failing to learn the correct distribution without it. By studying transformers on simple data generating processes, it was found that with proper tokenization, they can effectively model sequence probabilities from Markov sources, supporting the practical use of tokenization in language processing.\n",
      "http://arxiv.org/abs/2404.08417v1: [ True]\n",
      "Large language models can now handle complex tasks by recalling information from a pretraining corpus, but concerns arise regarding evolving data needs, such as introducing new data batches or managing user-based data access. AdapterSwap is introduced as a solution to organize knowledge into low-rank adapters, allowing for efficient continual learning and fine-grained control over data access and deletion.\n",
      "http://arxiv.org/abs/2404.08511v1: [ True]\n",
      "This study introduces a novel approach to cross-domain knowledge discovery by deploying specialized multi-AI agents that collaborate to provide comprehensive insights beyond single-domain expertise. Through comparative analysis, the research demonstrates the superior capability of domain-specific multi-AI agents in identifying and bridging knowledge gaps, highlighting the importance of collaborative AI in driving innovation and advancing cross-disciplinary research and applications.\n",
      "http://arxiv.org/abs/2404.08570v1: [ True]\n",
      "The CRITICAL framework enhances autonomous vehicle training and testing by generating diverse scenarios targeting specific learning gaps through real-world traffic dynamics and optional Large Language Model analysis. By establishing a closed feedback loop between data generation and training, CRITICAL improves learning rates, system performance, and safety resilience, showing potential to enhance AV systems' robustness and accelerate their development.\n",
      "http://arxiv.org/abs/2404.08727v1: [ True]\n",
      "This study evaluates the resource utilization and accuracy of Large Language Models (LLMs) in interpreting natural language queries against traditional SQL in relational database management systems, finding that even small LLMs incur significant energy overhead, making them environmentally unfriendly for database queries. The findings caution against replacing relational databases with LLMs due to their substantial resource utilization.\n",
      "http://arxiv.org/abs/2404.08743v1: [ True]\n",
      "Instructors use collaborative learning activities like Peer Instruction to engage students, but diverse mental models and ineffective collaboration can hinder success. VizGroup is an AI tool that helps programming instructors oversee real-time student collaboration during large courses by recommending event specifications based on collaboration metrics, leading to improved monitoring and notification strategies.\n",
      "http://arxiv.org/abs/2404.08811v1: [ True]\n",
      "The surge in demand for Machine Learning and Artificial Intelligence applications is straining the technology infrastructure, leading to unsustainable spending trends and hindering innovation. Addressing these challenges requires significant advancements in supercomputing, AI training approaches, hardware design, and energy efficiency to reduce barriers to entry for training large language models.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_preds)):\n",
    "    if new_preds[i] == True:\n",
    "        paper_id = new_ids[i][\"paper_id\"]\n",
    "        summary = new_ids[i][\"concise_summary\"]\n",
    "        print(f\"{paper_id}: {new_preds[i]}\\n{summary}\")\n",
    "        formatted.append({\"id\": paper_id, \"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8735822f-444a-423a-a098-4a758e39b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'http://arxiv.org/abs/2404.03502v1', 'summary': 'Artificial intelligence can boost productivity and generate insights, but its widespread use may have unintended consequences, such as harming public understanding. The reliance on recursive AI systems could lead to \"knowledge collapse,\" impacting innovation and human culture, as demonstrated by a model where a discount on AI-generated content leads to public beliefs further from the truth. Further research is needed to address and mitigate these outcomes.'}, {'id': 'http://arxiv.org/abs/2404.03558v1', 'summary': 'Large language models have demonstrated the ability to perform new tasks with limited examples through in-context learning, with multi-task learning offering promising transfer learning potential. This research explores combining multi-task learning with in-context learning to create models that efficiently learn tasks and are robust to out-of-distribution examples, proposing curriculum learning strategies for higher data efficiency and stable convergence. The experiments show that in-context learning models can effectively tackle difficult tasks by training on progressively harder tasks while mixing in prior tasks, enhancing generalization and robustness.'}, {'id': 'http://arxiv.org/abs/2404.03631v1', 'summary': \"The text discusses the use of Task Vectors (TV) for unconditionally erasing concepts from a text-to-image model, showing that TV-based erasure is more robust to unexpected user inputs but can impact the model's core performance. The proposed Diverse Inversion method helps estimate the required edit strength of the TV edit by finding diverse word embeddings that induce the generation of the target concept, allowing for selective erasure of model weights to enhance erasure capabilities while preserving core functionality.\"}, {'id': 'http://arxiv.org/abs/2404.03880v1', 'summary': 'Advancements in Machine Learning have led to various methods for analyzing unstructured data such as images and text, enabling meaningful information extraction and storage in relational databases for SQL queries. A novel framework called SSQL integrates semantic queries with SQL statements, optimizing queries by combining semantic predicates with structured table predicates and incorporating human-in-the-loop feedback for improved results.'}, {'id': 'http://arxiv.org/abs/2404.03995v1', 'summary': 'Recent advancements in AI technology have prompted companies to consider integrating AI into software systems for sustainability purposes, despite potential negative impacts. The study synthesized trade-offs related to sustainability to provide practitioners with transparent insights on the benefits and costs of AI integration, identifying key themes like energy management, employee wellbeing, deployment issues, ethics & society, and environmental issues.'}, {'id': 'http://arxiv.org/abs/2404.04204v1', 'summary': 'This perspective paper highlights barriers to social skill training and proposes leveraging large language models through an AI Mentor framework for more accessible and realistic practice with tailored feedback. The aim is to enhance social skill development across various fields and promote workforce development and social equality through cross-disciplinary innovation.'}, {'id': 'http://arxiv.org/abs/2404.04237v1', 'summary': 'Large language models (LLMs) have shown remarkable capabilities in surpassing human performance on diverse tasks but can struggle with simple tasks, emphasizing the need for improved evaluation methods. A study on compositional and conditional reasoning in LLMs, using a flight booking benchmark called GroundCocoa, revealed varying performance levels among state-of-the-art models, with the top-performing model achieving a maximum accuracy of 67%.'}, {'id': 'http://arxiv.org/abs/2404.04286v1', 'summary': 'Advancements in Large Language Models (LLMs) are leading to increased iterative interactions between models, with multi-round self-improving methods allowing for new example generation. By drawing parallels between LLM behavior and human cultural evolution using a Bayesian framework like Iterated Learning (IL), researchers aim to predict and guide the evolution of LLMs towards desired outcomes based on experimental verification.'}, {'id': 'http://arxiv.org/abs/2404.04289v1', 'summary': 'Researchers conducted a qualitative study on designing agents for negotiating tasks, finding that successful performance requires alignment over six dimensions, including knowledge schema, autonomy, training, reputation, ethics, and human engagement. These findings contribute to understanding the importance of aligning values and safety in Human-AI interactions, leading to proposed design directions for future Human-Agent collaborations.'}, {'id': 'http://arxiv.org/abs/2404.04500v1', 'summary': 'The text discusses the conflict between business secrecy and the societal need for transparency in algorithms. A protocol called ZkAudit is proposed to address this issue by allowing model providers to keep certain aspects secret while enabling trustless audits of model and data properties through cryptographic commitments and zero-knowledge proofs.'}, {'id': 'http://arxiv.org/abs/2404.04540v1', 'summary': 'Foundation models (FMs) have transformed various computing domains such as Automated Planning and Scheduling (APS) by aiding in tasks like plan generation, translation, and optimization. This paper emphasizes the importance of developing a comprehensive FM specifically for planning-like (PL) tasks like business processes and workflows to enhance problem-solving capabilities similar to how large language models (LLMs) are benefiting APS.'}, {'id': 'http://arxiv.org/abs/2404.04566v1', 'summary': 'Large Language Models (LLMs) have advanced software engineering tasks, leading to the emergence of the Large Language Models for Software Engineering (LLM4SE) field. This position paper advocates for prioritizing efficient and environmentally friendly LLM4SE solutions, proposing a roadmap for future research to achieve this goal and revolutionize the software engineering landscape.'}, {'id': 'http://arxiv.org/abs/2404.04570v1', 'summary': 'This text discusses the impact of large language models on AI system interaction patterns and highlights the lack of focus on human interaction with LLM in current literature reviews. The authors conducted a detailed review and mapping of 110 relevant publications, offering insights into human-LLM interaction patterns and identifying key challenges in this area.'}, {'id': 'http://arxiv.org/abs/2404.04572v1', 'summary': 'Sustainability is a key concern in Machine Learning enabled Systems, where ML models often struggle in production due to uncertainties like data variations and model instabilities. Machine Learning Operations (MLOps) aims to enhance adaptability in these systems, but faces challenges such as environmental impact and technical maintenance, which can be addressed through self-adaptive principles like the MAPE-K loop to improve sustainability and address uncertainties.'}, {'id': 'http://arxiv.org/abs/2404.04750v2', 'summary': 'The advancements in artificial intelligence present both opportunities and challenges for society, with the potential to significantly impact humanity akin to the industrial revolution. The One Hundred Year Study on AI, founded by multidisciplinary experts, emphasizes the importance of understanding AI models, addressing societal impacts, and fostering collaboration among stakeholders to guide the development of AI responsibly for human benefit.'}, {'id': 'http://arxiv.org/abs/2404.04821v1', 'summary': 'AI is reshaping Software Engineering with a focus on Automated Software Evolution for intelligent applications, presenting complexity from data heterogeneity and changing contexts. A proposed conceptual framework emphasizes multimodality learning and offers a Selective Sequential Scope Model (3S) for categorizing research across SE phases, serving as a practical guide for practitioners venturing into this evolving domain.'}, {'id': 'http://arxiv.org/abs/2404.04834v1', 'summary': 'This paper discusses the integration of Large Language Models (LLMs) into autonomous agents to enhance cognitive abilities in addressing complex software engineering challenges through LLM-based Multi-Agent (LMA) systems, emphasizing benefits like collaborative problem-solving and scalability. It envisions the role of LMA systems in future software engineering practices, highlighting potential applications, emerging challenges, and research opportunities to guide future directions.'}, {'id': 'http://arxiv.org/abs/2404.04839v1', 'summary': 'DevOps and security integration into the workflow is crucial, with AI-driven security approaches showing promise in automating security tasks to ensure uninterrupted delivery speed. A comprehensive analysis of AI-driven security techniques applicable to DevOps has identified key challenges and opportunities for enhancing security, trust, and efficiency in software development processes.'}, {'id': 'http://arxiv.org/abs/2404.04949v1', 'summary': 'Large language models are being used in specialized fields, facing challenges with diverse data leading to conflicts during task transfer. The Adaptive Semantic Space Learning framework improves multi-expert model performance by reorganizing data distributions, as shown by the financial LLM \"SilverSight\" achieving high results with minimal data and strong generalization abilities.'}, {'id': 'http://arxiv.org/abs/2404.05228v1', 'summary': 'This study focused on developing and evaluating an AI system to educate individuals on making unbiased decisions by using fairness-aware machine learning. Participants who received AI guidance for fair decision-making showed a willingness to reassess their views on fairness, reflect on biases, and adjust decision-making criteria, highlighting the potential of AI systems in promoting unbiased decision-making in humans.'}, {'id': 'http://arxiv.org/abs/2404.05365v1', 'summary': 'The paper discusses the marginalization of indigenous language communities in the face of technological advancements, emphasizing the importance of inclusive NLP innovations that respect their cultural richness. It highlights the NLP progress of indigenous Latin American languages and the challenges and innovations needed for their preservation and development, aiming to bridge the gap between these communities and researchers.'}, {'id': 'http://arxiv.org/abs/2404.05442v1', 'summary': 'This paper explores using ChatGPT, a generative AI tool, to develop user personas and adaptive interfaces as an alternative to traditional manual processes, showing promising results in improving the design process efficiency. By leveraging the power of Large Language Models, there is potential to streamline the time, effort, and cost associated with user research for user-centered applications.'}, {'id': 'http://arxiv.org/abs/2404.05449v1', 'summary': 'A framework called Reflection on search Trees (RoT) is introduced to enhance the performance of large language models (LLMs) in reasoning and planning tasks by summarizing guidelines from previous search experiences to prevent repeated mistakes. RoT significantly improves LLM performance with tree-search-based prompting methods like BFS and MCTS, and can also benefit non-tree-search-based methods by providing task-specific knowledge derived from search experiences.'}, {'id': 'http://arxiv.org/abs/2404.05569v1', 'summary': 'Recent advancements in large language model agents focus on optimizing agent teams and implementing self-reflection to improve performance in complex tasks. A proposed framework called 360°REA utilizes a multi-agent approach with a 360° performance assessment method to enhance agent capabilities through experience accumulation, showing effectiveness in addressing complex tasks based on experimental results with various datasets.'}, {'id': 'http://arxiv.org/abs/2404.05602v1', 'summary': 'This research proposes an AI-powered cyber incident response system for cloud environments, using AI and ML for Network Traffic Classification and Malware Analysis, achieving high accuracy rates of 90% and 96% respectively with the Random Forest model. The study emphasizes the efficiency and scalability of AI/ML systems in cloud environments, utilizing container technology and cloud-based TPUs and GPUs for managing resource demands and ensuring a robust cyber incident response solution.'}, {'id': 'http://arxiv.org/abs/2404.05779v1', 'summary': 'Data quality is vital for effective AI models, with poor data leading to inaccuracies and potential risks. Current research aims to establish standardized metrics for evaluating data readiness to improve the quality and accuracy of AI training and inference.'}, {'id': 'http://arxiv.org/abs/2404.05874v1', 'summary': 'This study involved a two-week workshop where 13 youth aged 14-15 designed and audited ML-powered applications, leading to increased awareness of algorithmic biases and model design issues among participants. The findings suggest that involving youth in auditing algorithms can enhance their understanding of model functionality and inspire improvements in their own applications, contributing to research on child-computer interaction and learning.'}, {'id': 'http://arxiv.org/abs/2404.05904v1', 'summary': 'Large Language Models (LLMs) are powerful in NLP but can produce inaccurate outputs known as \"hallucinations.\" The Hallucinations Leaderboard quantitatively assesses different models\\' tendency to create hallucinations across various NLP tasks, helping researchers and practitioners select more reliable models for their needs.'}, {'id': 'http://arxiv.org/abs/2404.05990v1', 'summary': 'Rapid advancements in Artificial Intelligence have led to increased power exerted over individuals through automated systems, impacting various aspects of society from government services to information dissemination and economic interactions. Scholars are advocating for a critical examination of these new power dynamics created by AI technologies to ensure proper authority and procedural legitimacy in their use.'}, {'id': 'http://arxiv.org/abs/2404.06201v1', 'summary': 'Large Language Models (LLMs) are crucial for improving software engineering tasks by enhancing code understanding, but their collaboration relies heavily on accessing high-quality data, which can be restricted due to commercial and privacy concerns, hindering progress in open-source AI-based software engineering projects. To address this challenge, a governance framework centered on federated learning (FL) is proposed to enable open-source AI models to leverage diverse organizational resources while ensuring data privacy and security, along with guidelines for developers on collaboration aspects such as data requirements, model architecture, updating strategies, and version control.'}, {'id': 'http://arxiv.org/abs/2404.06404v1', 'summary': 'Large Language Models (LLMs) are powerful tools in research, offering benefits like cost-effectiveness and efficiency, but they also pose challenges such as prompt tuning, biases, and subjectivity that need to be addressed. This study explores the potential of LLMs through literature review and experimentation, highlighting successes, limitations, and strategies for mitigating challenges to integrate them into research ethically.'}, {'id': 'http://arxiv.org/abs/2404.06411v1', 'summary': 'Advances in Large Language Models have spurred the development of LLM agents for complex reasoning tasks, emphasizing the importance of benchmarking and evaluation for progress. The AgentQuest framework offers modular benchmarks and new evaluation metrics to track LLM agent progress effectively, highlighting common failure points and enhancing performance through refined architecture, aiming for community collaboration and extension.'}, {'id': 'http://arxiv.org/abs/2404.06664v1', 'summary': \"Current methods for evaluating the multicultural knowledge of large language models struggle to capture the complexity and diversity of cultural norms, potentially perpetuating biases in their benchmarks. An interactive system called CulturalTeaming leverages human-AI collaboration to create challenging evaluation datasets that expose shortcomings in modern LLMs' multicultural proficiency, with accuracy ranging from 37.7% to 72.2% across different LLM families.\"}, {'id': 'http://arxiv.org/abs/2404.06777v1', 'summary': 'Integrating AI and federated learning in smart transportation raises concerns about responsible use, crucial for system stability and sustainability. Research on the responsible application of AI and FL in this area is limited, with a need for more thorough exploration and solutions to challenges in developing and implementing responsible FL.'}, {'id': 'http://arxiv.org/abs/2404.06948v1', 'summary': 'The paper introduces a winning solution for the SemEval-2024 Task 6 competition, utilizing a meta-regressor framework of large language models (LLMs) to evaluate and integrate models effectively, achieving top scores on the leader board. By harnessing uncertainty signals from a variety of LLMs, the approach enhances the detection of hallucinations with greater robustness.'}, {'id': 'http://arxiv.org/abs/2404.06954v1', 'summary': 'Dynamic computation methods for Large Language Models have sped up processing by skipping layers through heuristics or predictors, but existing decoding processes assign different computational budgets to samples, leading to instability. The proposed Unified Layer Skipping strategy selects the number of layers to skip based on a speedup ratio, balancing computation skipping to improve inference performance and model throughput for tasks like machine translation and text summarization.'}, {'id': 'http://arxiv.org/abs/2404.07066v1', 'summary': 'This paper investigates how large language models learn different concepts across their layers, with deeper layers handling more complex and abstract concepts. Through a probing technique, it shows that simpler tasks are efficiently classified in shallower layers, while deeper layers are crucial for discerning more complex tasks, impacting our understanding of model learning processes and representations.'}, {'id': 'http://arxiv.org/abs/2404.07142v1', 'summary': 'Software systems are crucial in modern society, but the diversity of software development teams often does not reflect the user base. To ensure inclusive work environments and usable software for diverse populations, efforts are needed to promote software developer diversity and inclusion, especially as technology like AI influences the landscape of software engineering.'}, {'id': 'http://arxiv.org/abs/2404.07413v1', 'summary': 'The JetMoE-8B is a cost-effective Large Language Model trained with minimal resources, showcasing impressive performance and outperforming other models, indicating that LLM training can be more affordable than previously believed. JetMoE-8B utilizes an efficient Sparsely-gated Mixture-of-Experts architecture, with sparsely activated layers reducing inference computation by about 70% compared to other models, and it promotes transparency, collaboration, and advancements in accessible and efficient LLM development.'}, {'id': 'http://arxiv.org/abs/2404.07461v1', 'summary': 'A study examines how hallucination is understood in large language models by analyzing 103 research papers and conducting a survey with 171 NLP and AI experts, revealing a lack of consensus on the term and the need for clear definitions and frameworks in the field. The research highlights the importance of defining hallucination in NLP, emphasizing potential challenges and societal impacts, based on insights gathered from practitioners in the field.'}, {'id': 'http://arxiv.org/abs/2404.07501v1', 'summary': 'Research in business process modeling explores automated generation of process models from data, such as event logs and natural language texts, aiming to reduce costs and improve efficiency. This study investigates using data augmentation techniques from machine learning to enhance accuracy in extracting business process information from text, showing promising results in improving $F_1$ scores for mention and relation extraction. The findings suggest that data augmentation plays a crucial role in advancing machine learning methods for business process model generation from natural language text, offering insights through visualization and analysis of augmented textual data.'}, {'id': 'http://arxiv.org/abs/2404.07678v1', 'summary': \"To thrive, organizations must prioritize innovation and address ethical and sustainability concerns, especially in today's fast-paced environment. This involves integrating ethical and sustainability considerations, particularly in the adoption of emerging technologies like artificial intelligence, to foster innovation cultures in both startups and established enterprises using approaches such as systems thinking.\"}, {'id': 'http://arxiv.org/abs/2404.07738v1', 'summary': 'A ResearchAgent powered by a language model is proposed to streamline scientific research by automatically generating and refining research ideas, methods, and experiment designs through iterative processes based on existing literature. By connecting information from academic graphs and entity-centric knowledge stores, as well as incorporating peer review feedback, the ResearchAgent demonstrates effectiveness in producing innovative and valid research ideas across various disciplines.'}, {'id': 'http://arxiv.org/abs/2404.08335v1', 'summary': 'Research has shown that tokenization is essential for creating high-performing language models, as demonstrated by transformers failing to learn the correct distribution without it. By studying transformers on simple data generating processes, it was found that with proper tokenization, they can effectively model sequence probabilities from Markov sources, supporting the practical use of tokenization in language processing.'}, {'id': 'http://arxiv.org/abs/2404.08417v1', 'summary': 'Large language models can now handle complex tasks by recalling information from a pretraining corpus, but concerns arise regarding evolving data needs, such as introducing new data batches or managing user-based data access. AdapterSwap is introduced as a solution to organize knowledge into low-rank adapters, allowing for efficient continual learning and fine-grained control over data access and deletion.'}, {'id': 'http://arxiv.org/abs/2404.08511v1', 'summary': 'This study introduces a novel approach to cross-domain knowledge discovery by deploying specialized multi-AI agents that collaborate to provide comprehensive insights beyond single-domain expertise. Through comparative analysis, the research demonstrates the superior capability of domain-specific multi-AI agents in identifying and bridging knowledge gaps, highlighting the importance of collaborative AI in driving innovation and advancing cross-disciplinary research and applications.'}, {'id': 'http://arxiv.org/abs/2404.08570v1', 'summary': \"The CRITICAL framework enhances autonomous vehicle training and testing by generating diverse scenarios targeting specific learning gaps through real-world traffic dynamics and optional Large Language Model analysis. By establishing a closed feedback loop between data generation and training, CRITICAL improves learning rates, system performance, and safety resilience, showing potential to enhance AV systems' robustness and accelerate their development.\"}, {'id': 'http://arxiv.org/abs/2404.08727v1', 'summary': 'This study evaluates the resource utilization and accuracy of Large Language Models (LLMs) in interpreting natural language queries against traditional SQL in relational database management systems, finding that even small LLMs incur significant energy overhead, making them environmentally unfriendly for database queries. The findings caution against replacing relational databases with LLMs due to their substantial resource utilization.'}, {'id': 'http://arxiv.org/abs/2404.08743v1', 'summary': 'Instructors use collaborative learning activities like Peer Instruction to engage students, but diverse mental models and ineffective collaboration can hinder success. VizGroup is an AI tool that helps programming instructors oversee real-time student collaboration during large courses by recommending event specifications based on collaboration metrics, leading to improved monitoring and notification strategies.'}, {'id': 'http://arxiv.org/abs/2404.08811v1', 'summary': 'The surge in demand for Machine Learning and Artificial Intelligence applications is straining the technology infrastructure, leading to unsustainable spending trends and hindering innovation. Addressing these challenges requires significant advancements in supercomputing, AI training approaches, hardware design, and energy efficiency to reduce barriers to entry for training large language models.'}]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(new_preds)):\n",
    "#     if new_preds[i] == True:\n",
    "#         print(f'{new_ids[i][\"paper_id\"]}')\n",
    "\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4835ea91-e2bd-485a-ae74-cf31c3fe11ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2404.03502v1: AI and the Problem of Knowledge Collapse\n",
      "http://arxiv.org/abs/2404.03558v1: How does Multi-Task Training Affect Transformer In-Context Capabilities?\n",
      "  Investigations with Function Classes\n",
      "http://arxiv.org/abs/2404.03631v1: Robust Concept Erasure Using Task Vectors\n",
      "http://arxiv.org/abs/2404.03880v1: Semantic SQL -- Combining and optimizing semantic predicates in SQL\n",
      "http://arxiv.org/abs/2404.03995v1: Balancing Progress and Responsibility: A Synthesis of Sustainability\n",
      "  Trade-Offs of AI-Based Systems\n",
      "http://arxiv.org/abs/2404.04204v1: Social Skill Training with Large Language Models\n",
      "http://arxiv.org/abs/2404.04237v1: Cleared for Takeoff? Compositional & Conditional Reasoning may be the\n",
      "  Achilles Heel to (Flight-Booking) Language Agents\n",
      "http://arxiv.org/abs/2404.04286v1: Language Model Evolution: An Iterated Learning Perspective\n",
      "http://arxiv.org/abs/2404.04289v1: Designing for Human-Agent Alignment: Understanding what humans want from\n",
      "  their agents\n",
      "http://arxiv.org/abs/2404.04500v1: Trustless Audits without Revealing Data or Models\n",
      "http://arxiv.org/abs/2404.04540v1: The Case for Developing a Foundation Model for Planning-like Tasks from\n",
      "  Scratch\n",
      "http://arxiv.org/abs/2404.04566v1: Efficient and Green Large Language Models for Software Engineering:\n",
      "  Vision and the Road Ahead\n",
      "http://arxiv.org/abs/2404.04570v1: A Map of Exploring Human Interaction patterns with LLM: Insights into\n",
      "  Collaboration and Creativity\n",
      "http://arxiv.org/abs/2404.04572v1: Towards Architecting Sustainable MLOps: A Self-Adaptation Approach\n",
      "http://arxiv.org/abs/2404.04750v2: Now, Later, and Lasting: Ten Priorities for AI Research, Policy, and\n",
      "  Practice\n",
      "http://arxiv.org/abs/2404.04821v1: A Data-to-Product Multimodal Conceptual Framework to Achieve Automated\n",
      "  Software Evolution for Context-rich Intelligent Applications\n",
      "http://arxiv.org/abs/2404.04834v1: LLM-Based Multi-Agent Systems for Software Engineering: Vision and the\n",
      "  Road Ahead\n",
      "http://arxiv.org/abs/2404.04839v1: AI for DevSecOps: A Landscape and Future Opportunities\n",
      "http://arxiv.org/abs/2404.04949v1: SilverSight: A Multi-Task Chinese Financial Large Language Model Based\n",
      "  on Adaptive Semantic Space Learning\n",
      "http://arxiv.org/abs/2404.05228v1: Fair Machine Guidance to Enhance Fair Decision Making in Biased People\n",
      "http://arxiv.org/abs/2404.05365v1: NLP Progress in Indigenous Latin American Languages\n",
      "http://arxiv.org/abs/2404.05442v1: Unlocking Adaptive User Experience with Generative AI\n",
      "http://arxiv.org/abs/2404.05449v1: RoT: Enhancing Large Language Models with Reflection on Search Trees\n",
      "http://arxiv.org/abs/2404.05569v1: 360°REA: Towards A Reusable Experience Accumulation with 360°\n",
      "  Assessment for Multi-Agent System\n",
      "http://arxiv.org/abs/2404.05602v1: AI-Enabled System for Efficient and Effective Cyber Incident Detection\n",
      "  and Response in Cloud Environments\n",
      "http://arxiv.org/abs/2404.05779v1: Data Readiness for AI: A 360-Degree Survey\n",
      "http://arxiv.org/abs/2404.05874v1: Youth as Peer Auditors: Engaging Teenagers with Algorithm Auditing of\n",
      "  Machine Learning Applications\n",
      "http://arxiv.org/abs/2404.05904v1: The Hallucinations Leaderboard -- An Open Effort to Measure\n",
      "  Hallucinations in Large Language Models\n",
      "http://arxiv.org/abs/2404.05990v1: Automatic Authorities: Power and AI\n",
      "http://arxiv.org/abs/2404.06201v1: Open-Source AI-based SE Tools: Opportunities and Challenges of\n",
      "  Collaborative Software Learning\n",
      "http://arxiv.org/abs/2404.06404v1: Apprentices to Research Assistants: Advancing Research with Large\n",
      "  Language Models\n",
      "http://arxiv.org/abs/2404.06411v1: AgentQuest: A Modular Benchmark Framework to Measure Progress and\n",
      "  Improve LLM Agents\n",
      "http://arxiv.org/abs/2404.06664v1: CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging\n",
      "  LLMs' (Lack of) Multicultural Knowledge\n",
      "http://arxiv.org/abs/2404.06777v1: Responsible Federated Learning in Smart Transportation: Outlooks and\n",
      "  Challenges\n",
      "http://arxiv.org/abs/2404.06948v1: MetaCheckGPT -- A Multi-task Hallucination Detection Using LLM\n",
      "  Uncertainty and Meta-models\n",
      "http://arxiv.org/abs/2404.06954v1: Accelerating Inference in Large Language Models with a Unified Layer\n",
      "  Skipping Strategy\n",
      "http://arxiv.org/abs/2404.07066v1: Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n",
      "  Different Layers?\n",
      "http://arxiv.org/abs/2404.07142v1: Bridging Gaps, Building Futures: Advancing Software Developer Diversity\n",
      "  and Inclusion Through Future-Oriented Research\n",
      "http://arxiv.org/abs/2404.07413v1: JetMoE: Reaching Llama2 Performance with 0.1M Dollars\n",
      "http://arxiv.org/abs/2404.07461v1: \"Confidently Nonsensical?'': A Critical Survey on the Perspectives and\n",
      "  Challenges of 'Hallucinations' in NLP\n",
      "http://arxiv.org/abs/2404.07501v1: Leveraging Data Augmentation for Process Information Extraction\n",
      "http://arxiv.org/abs/2404.07678v1: On the role of ethics and sustainability in business innovation\n",
      "http://arxiv.org/abs/2404.07738v1: ResearchAgent: Iterative Research Idea Generation over Scientific\n",
      "  Literature with Large Language Models\n",
      "http://arxiv.org/abs/2404.08335v1: Toward a Theory of Tokenization in LLMs\n",
      "http://arxiv.org/abs/2404.08417v1: AdapterSwap: Continuous Training of LLMs with Data Removal and\n",
      "  Access-Control Guarantees\n",
      "http://arxiv.org/abs/2404.08511v1: Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery\n",
      "http://arxiv.org/abs/2404.08570v1: Enhancing Autonomous Vehicle Training with Language Model Integration\n",
      "  and Critical Scenario Generation\n",
      "http://arxiv.org/abs/2404.08727v1: Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs\n",
      "  versus Traditional Relational Databases\n",
      "http://arxiv.org/abs/2404.08743v1: VizGroup: An AI-Assisted Event-Driven System for Real-Time Collaborative\n",
      "  Programming Learning Analytics\n",
      "http://arxiv.org/abs/2404.08811v1: Reducing the Barriers to Entry for Foundation Model Training\n"
     ]
    }
   ],
   "source": [
    "# Retrieve article titles\n",
    "\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, \"/home/woojay/P/ML/arxiver\")\n",
    "\n",
    "from arxiver.database import create_connection\n",
    "\n",
    "conn = create_connection(\"../data/arxiv_papers.db\")\n",
    "\n",
    "if conn is not None:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for i in range(len(new_preds)):\n",
    "        if new_preds[i] == True:\n",
    "            # Fetch the specific entry\n",
    "            cursor.execute(\n",
    "                \"SELECT paper_id, title, summary, concise_summary FROM papers WHERE paper_id = ?\",\n",
    "                (new_ids[i][\"paper_id\"],),\n",
    "            )\n",
    "            entry = cursor.fetchone()\n",
    "\n",
    "            if not entry:\n",
    "                conn.close()\n",
    "                raise HTTPException(status_code=404, detail=\"Paper not found\")\n",
    "\n",
    "            paper_id, title, summary, concise_summary = entry\n",
    "\n",
    "            print(f\"{paper_id}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0110be7f-f5d9-48b9-abdd-af959e8dfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask openAI to pick the best articles:\n",
    "\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def choose_summaries(summaries, k):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert summarizer capable of distilling complex information into its essence and a skilled evaluator of cutting edge ideas. Your choices should be based on the most interesting, novel, and cutting edge ideas.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"From the following article summaries, pick the {k} most interesting, novel, and cutting edge ideas and return a json list with 'id' and 'summary' for each. The id should contain the article id. You may also include a 'reason' for each choice.: {summaries}\",\n",
    "                },\n",
    "            ],\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        response_content = (\n",
    "            response.choices[0]\n",
    "            .message.content.strip(\"`\")\n",
    "            .strip()\n",
    "            .removeprefix(\"json\\n\")\n",
    "        )\n",
    "\n",
    "        # Debugging\n",
    "        # print(\"Raw response content:\", response_content)\n",
    "\n",
    "        if response_content:\n",
    "            parsed_response = json.loads(response_content)\n",
    "            return parsed_response\n",
    "        else:\n",
    "            print(\"Response content is empty.\")\n",
    "            return []\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Failed to decode JSON:\", e)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa4726e1-d9b5-4f7f-84b1-3b72aad43544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"id\": \"http://arxiv.org/abs/2404.03502v1\",\n",
      "    \"summary\": \"Artificial intelligence can boost productivity and generate insights, but its widespread use may have unintended consequences, such as harming public understanding. The reliance on recursive AI systems could lead to 'knowledge collapse,' impacting innovation and human culture, as demonstrated by a model where a discount on AI-generated content leads to public beliefs further from the truth. Further research is needed to address and mitigate these outcomes.\",\n",
      "    \"reason\": \"The concept of 'knowledge collapse' due to AI is a novel and critical issue that could have profound implications on society and culture. It's a cutting-edge idea that challenges the current trajectory of AI development and calls for a deeper investigation into the long-term effects of AI on human knowledge and understanding.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"http://arxiv.org/abs/2404.04286v1\",\n",
      "    \"summary\": \"Advancements in Large Language Models (LLMs) are leading to increased iterative interactions between models, with multi-round self-improving methods allowing for new example generation. By drawing parallels between LLM behavior and human cultural evolution using a Bayesian framework like Iterated Learning (IL), researchers aim to predict and guide the evolution of LLMs towards desired outcomes based on experimental verification.\",\n",
      "    \"reason\": \"The parallel drawn between the evolution of LLMs and human cultural evolution is a fascinating and innovative approach to understanding and guiding AI development. This idea is at the forefront of AI research, offering a new perspective on how AI systems learn and evolve over time.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"http://arxiv.org/abs/2404.04500v1\",\n",
      "    \"summary\": \"The text discusses the conflict between business secrecy and the societal need for transparency in algorithms. A protocol called ZkAudit is proposed to address this issue by allowing model providers to keep certain aspects secret while enabling trustless audits of model and data properties through cryptographic commitments and zero-knowledge proofs.\",\n",
      "    \"reason\": \"ZkAudit is a cutting-edge solution that addresses the critical balance between the need for business confidentiality and the public's demand for transparency in AI algorithms. The use of cryptographic techniques to enable trustless audits is a novel approach that could revolutionize the way AI models are audited and trusted.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"http://arxiv.org/abs/2404.07413v1\",\n",
      "    \"summary\": \"The JetMoE-8B is a cost-effective Large Language Model trained with minimal resources, showcasing impressive performance and outperforming other models, indicating that LLM training can be more affordable than previously believed. JetMoE-8B utilizes an efficient Sparsely-gated Mixture-of-Experts architecture, with sparsely activated layers reducing inference computation by about 70% compared to other models, and it promotes transparency, collaboration, and advancements in accessible and efficient LLM development.\",\n",
      "    \"reason\": \"The development of a cost-effective and efficient LLM like JetMoE-8B is a groundbreaking achievement that could democratize access to advanced AI technologies. The significant reduction in inference computation and the promotion of transparency and collaboration are key factors that make this idea stand out in the field of AI.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"http://arxiv.org/abs/2404.08511v1\",\n",
      "    \"summary\": \"This study introduces a novel approach to cross-domain knowledge discovery by deploying specialized multi-AI agents that collaborate to provide comprehensive insights beyond single-domain expertise. Through comparative analysis, the research demonstrates the superior capability of domain-specific multi-AI agents in identifying and bridging knowledge gaps, highlighting the importance of collaborative AI in driving innovation and advancing cross-disciplinary research and applications.\",\n",
      "    \"reason\": \"The concept of multi-AI agents collaborating to achieve cross-domain knowledge discovery is a novel and exciting development in AI research. This approach has the potential to significantly enhance the capability of AI systems to provide comprehensive insights and drive innovation across various fields.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "picks = choose_summaries(formatted, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4fe38810-0cfe-4e4d-8a01-8acf67d95553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e67d15d-c069-4f35-8448-c60dff599d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "new_model = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            320,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train.shape[1],),\n",
    "            kernel_regularizer=l2(0.001),\n",
    "        ),\n",
    "        Dropout(0.0),\n",
    "        BatchNormalization(),\n",
    "        Dense(224, activation=\"relu\", kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9b976dd0-7b4d-4891-bac7-d8cdc925cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 2.2305e-05 - accuracy: 1.0000 - val_loss: 0.7483 - val_accuracy: 0.9447\n",
      "Epoch 2/30\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 2.6119e-05 - accuracy: 1.0000 - val_loss: 0.7524 - val_accuracy: 0.9447\n",
      "Epoch 3/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 2.2027e-05 - accuracy: 1.0000 - val_loss: 0.7528 - val_accuracy: 0.9447\n",
      "Epoch 4/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 3.0896e-05 - accuracy: 1.0000 - val_loss: 0.7578 - val_accuracy: 0.9447\n",
      "Epoch 5/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 2.3630e-05 - accuracy: 1.0000 - val_loss: 0.7579 - val_accuracy: 0.9447\n",
      "Epoch 6/30\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 1.8129e-05 - accuracy: 1.0000 - val_loss: 0.7603 - val_accuracy: 0.9447\n",
      "Epoch 7/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 2.3235e-05 - accuracy: 1.0000 - val_loss: 0.7587 - val_accuracy: 0.9401\n",
      "Epoch 8/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 2.1698e-05 - accuracy: 1.0000 - val_loss: 0.7691 - val_accuracy: 0.9447\n",
      "Epoch 9/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.8839e-05 - accuracy: 1.0000 - val_loss: 0.7708 - val_accuracy: 0.9447\n",
      "Epoch 10/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.9842e-05 - accuracy: 1.0000 - val_loss: 0.7703 - val_accuracy: 0.9424\n",
      "Epoch 11/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 1.6478e-05 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.9447\n",
      "Epoch 12/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 1.4612e-05 - accuracy: 1.0000 - val_loss: 0.7789 - val_accuracy: 0.9424\n",
      "Epoch 13/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.7764e-05 - accuracy: 1.0000 - val_loss: 0.7860 - val_accuracy: 0.9447\n",
      "Epoch 14/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.3596e-05 - accuracy: 1.0000 - val_loss: 0.7883 - val_accuracy: 0.9447\n",
      "Epoch 15/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.1428e-05 - accuracy: 1.0000 - val_loss: 0.7851 - val_accuracy: 0.9447\n",
      "Epoch 16/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.2878e-05 - accuracy: 1.0000 - val_loss: 0.7872 - val_accuracy: 0.9447\n",
      "Epoch 17/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 1.1280e-05 - accuracy: 1.0000 - val_loss: 0.7904 - val_accuracy: 0.9447\n",
      "Epoch 18/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 1.1499e-05 - accuracy: 1.0000 - val_loss: 0.7976 - val_accuracy: 0.9447\n",
      "Epoch 19/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 1.2000e-05 - accuracy: 1.0000 - val_loss: 0.7943 - val_accuracy: 0.9424\n",
      "Epoch 20/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.4914e-05 - accuracy: 1.0000 - val_loss: 0.7948 - val_accuracy: 0.9401\n",
      "Epoch 21/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.0582e-05 - accuracy: 1.0000 - val_loss: 0.7983 - val_accuracy: 0.9424\n",
      "Epoch 22/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 1.8195e-05 - accuracy: 1.0000 - val_loss: 0.8103 - val_accuracy: 0.9447\n",
      "Epoch 23/30\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 9.2116e-06 - accuracy: 1.0000 - val_loss: 0.8073 - val_accuracy: 0.9447\n",
      "Epoch 24/30\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 1.1160e-05 - accuracy: 1.0000 - val_loss: 0.8029 - val_accuracy: 0.9447\n",
      "Epoch 25/30\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 1.6644e-05 - accuracy: 1.0000 - val_loss: 0.8085 - val_accuracy: 0.9447\n",
      "Epoch 26/30\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 9.3507e-06 - accuracy: 1.0000 - val_loss: 0.8091 - val_accuracy: 0.9447\n",
      "Epoch 27/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.1067e-05 - accuracy: 1.0000 - val_loss: 0.8210 - val_accuracy: 0.9447\n",
      "Epoch 28/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 1.2936e-05 - accuracy: 1.0000 - val_loss: 0.8152 - val_accuracy: 0.9424\n",
      "Epoch 29/30\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 2.6164e-05 - accuracy: 1.0000 - val_loss: 0.8058 - val_accuracy: 0.9424\n",
      "Epoch 30/30\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.6825 - val_accuracy: 0.9378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4061106d50>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7afa1df4-69c8-439f-b489-ee0427e0a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted articles\n",
    "\n",
    "new_x = []\n",
    "formatted = []\n",
    "for id in new_ids:\n",
    "    new_x.append(get_embedding(id[\"paper_id\"]))\n",
    "\n",
    "new_preds = model.predict(new_x) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4711747c-8c62-492c-b450-14e4dd6f1e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2404.03502v1: [ True]\n",
      "Artificial intelligence can boost productivity and generate insights, but its widespread use may have unintended consequences, such as harming public understanding. The reliance on recursive AI systems could lead to \"knowledge collapse,\" impacting innovation and human culture, as demonstrated by a model where a discount on AI-generated content leads to public beliefs further from the truth. Further research is needed to address and mitigate these outcomes.\n",
      "http://arxiv.org/abs/2404.03631v1: [ True]\n",
      "The text discusses the use of Task Vectors (TV) for unconditionally erasing concepts from a text-to-image model, showing that TV-based erasure is more robust to unexpected user inputs but can impact the model's core performance. The proposed Diverse Inversion method helps estimate the required edit strength of the TV edit by finding diverse word embeddings that induce the generation of the target concept, allowing for selective erasure of model weights to enhance erasure capabilities while preserving core functionality.\n",
      "http://arxiv.org/abs/2404.03683v1: [ True]\n",
      "This paper introduces a method for teaching language models to search by representing the search process as a flattened string (SoS). By pretraining a transformer-based language model on streams of search generated by heuristic solvers and finetuning it with policy improvement methods, the SoS models demonstrate improved search accuracy and solve previously unsolved problems, showcasing their ability to learn problem-solving through search strategies.\n",
      "http://arxiv.org/abs/2404.03880v1: [ True]\n",
      "Advancements in Machine Learning have led to various methods for analyzing unstructured data such as images and text, enabling meaningful information extraction and storage in relational databases for SQL queries. A novel framework called SSQL integrates semantic queries with SQL statements, optimizing queries by combining semantic predicates with structured table predicates and incorporating human-in-the-loop feedback for improved results.\n",
      "http://arxiv.org/abs/2404.03995v1: [ True]\n",
      "Recent advancements in AI technology have prompted companies to consider integrating AI into software systems for sustainability purposes, despite potential negative impacts. The study synthesized trade-offs related to sustainability to provide practitioners with transparent insights on the benefits and costs of AI integration, identifying key themes like energy management, employee wellbeing, deployment issues, ethics & society, and environmental issues.\n",
      "http://arxiv.org/abs/2404.04237v1: [ True]\n",
      "Large language models (LLMs) have shown remarkable capabilities in surpassing human performance on diverse tasks but can struggle with simple tasks, emphasizing the need for improved evaluation methods. A study on compositional and conditional reasoning in LLMs, using a flight booking benchmark called GroundCocoa, revealed varying performance levels among state-of-the-art models, with the top-performing model achieving a maximum accuracy of 67%.\n",
      "http://arxiv.org/abs/2404.04286v1: [ True]\n",
      "Advancements in Large Language Models (LLMs) are leading to increased iterative interactions between models, with multi-round self-improving methods allowing for new example generation. By drawing parallels between LLM behavior and human cultural evolution using a Bayesian framework like Iterated Learning (IL), researchers aim to predict and guide the evolution of LLMs towards desired outcomes based on experimental verification.\n",
      "http://arxiv.org/abs/2404.04289v1: [ True]\n",
      "Researchers conducted a qualitative study on designing agents for negotiating tasks, finding that successful performance requires alignment over six dimensions, including knowledge schema, autonomy, training, reputation, ethics, and human engagement. These findings contribute to understanding the importance of aligning values and safety in Human-AI interactions, leading to proposed design directions for future Human-Agent collaborations.\n",
      "http://arxiv.org/abs/2404.04344v1: [ True]\n",
      "Theoretical development and investigation in formal concept analysis rely heavily on data sets, but the lack of a centralized repository for these data sets hinders sustainable research progress. This article examines the challenges of data set distribution in FCA research and outlines the need for a central repository linking data sets to analysis results.\n",
      "http://arxiv.org/abs/2404.04500v1: [ True]\n",
      "The text discusses the conflict between business secrecy and the societal need for transparency in algorithms. A protocol called ZkAudit is proposed to address this issue by allowing model providers to keep certain aspects secret while enabling trustless audits of model and data properties through cryptographic commitments and zero-knowledge proofs.\n",
      "http://arxiv.org/abs/2404.04540v1: [ True]\n",
      "Foundation models (FMs) have transformed various computing domains such as Automated Planning and Scheduling (APS) by aiding in tasks like plan generation, translation, and optimization. This paper emphasizes the importance of developing a comprehensive FM specifically for planning-like (PL) tasks like business processes and workflows to enhance problem-solving capabilities similar to how large language models (LLMs) are benefiting APS.\n",
      "http://arxiv.org/abs/2404.04566v1: [ True]\n",
      "Large Language Models (LLMs) have advanced software engineering tasks, leading to the emergence of the Large Language Models for Software Engineering (LLM4SE) field. This position paper advocates for prioritizing efficient and environmentally friendly LLM4SE solutions, proposing a roadmap for future research to achieve this goal and revolutionize the software engineering landscape.\n",
      "http://arxiv.org/abs/2404.04570v1: [ True]\n",
      "This text discusses the impact of large language models on AI system interaction patterns and highlights the lack of focus on human interaction with LLM in current literature reviews. The authors conducted a detailed review and mapping of 110 relevant publications, offering insights into human-LLM interaction patterns and identifying key challenges in this area.\n",
      "http://arxiv.org/abs/2404.04572v1: [ True]\n",
      "Sustainability is a key concern in Machine Learning enabled Systems, where ML models often struggle in production due to uncertainties like data variations and model instabilities. Machine Learning Operations (MLOps) aims to enhance adaptability in these systems, but faces challenges such as environmental impact and technical maintenance, which can be addressed through self-adaptive principles like the MAPE-K loop to improve sustainability and address uncertainties.\n",
      "http://arxiv.org/abs/2404.04750v2: [ True]\n",
      "The advancements in artificial intelligence present both opportunities and challenges for society, with the potential to significantly impact humanity akin to the industrial revolution. The One Hundred Year Study on AI, founded by multidisciplinary experts, emphasizes the importance of understanding AI models, addressing societal impacts, and fostering collaboration among stakeholders to guide the development of AI responsibly for human benefit.\n",
      "http://arxiv.org/abs/2404.04821v1: [ True]\n",
      "AI is reshaping Software Engineering with a focus on Automated Software Evolution for intelligent applications, presenting complexity from data heterogeneity and changing contexts. A proposed conceptual framework emphasizes multimodality learning and offers a Selective Sequential Scope Model (3S) for categorizing research across SE phases, serving as a practical guide for practitioners venturing into this evolving domain.\n",
      "http://arxiv.org/abs/2404.04834v1: [ True]\n",
      "This paper discusses the integration of Large Language Models (LLMs) into autonomous agents to enhance cognitive abilities in addressing complex software engineering challenges through LLM-based Multi-Agent (LMA) systems, emphasizing benefits like collaborative problem-solving and scalability. It envisions the role of LMA systems in future software engineering practices, highlighting potential applications, emerging challenges, and research opportunities to guide future directions.\n",
      "http://arxiv.org/abs/2404.04839v1: [ True]\n",
      "DevOps and security integration into the workflow is crucial, with AI-driven security approaches showing promise in automating security tasks to ensure uninterrupted delivery speed. A comprehensive analysis of AI-driven security techniques applicable to DevOps has identified key challenges and opportunities for enhancing security, trust, and efficiency in software development processes.\n",
      "http://arxiv.org/abs/2404.04949v1: [ True]\n",
      "Large language models are being used in specialized fields, facing challenges with diverse data leading to conflicts during task transfer. The Adaptive Semantic Space Learning framework improves multi-expert model performance by reorganizing data distributions, as shown by the financial LLM \"SilverSight\" achieving high results with minimal data and strong generalization abilities.\n",
      "http://arxiv.org/abs/2404.05089v1: [ True]\n",
      "Advancements in deep learning have led to Mixture-of-Experts (MoEs) models, which dynamically allocate resources based on input, but face challenges with memory requirements. The SEER-MoE framework introduces a two-stage approach involving expert pruning and fine-tuning to reduce memory footprint and compute requirements of pre-trained MoE models, optimizing inference efficiency while minimizing accuracy trade-offs.\n",
      "http://arxiv.org/abs/2404.05228v1: [ True]\n",
      "This study focused on developing and evaluating an AI system to educate individuals on making unbiased decisions by using fairness-aware machine learning. Participants who received AI guidance for fair decision-making showed a willingness to reassess their views on fairness, reflect on biases, and adjust decision-making criteria, highlighting the potential of AI systems in promoting unbiased decision-making in humans.\n",
      "http://arxiv.org/abs/2404.05270v1: [ True]\n",
      "The study explores two interaction paradigms for Algorithmic Recourse in decision-making, finding that users perceive the guided approach as more efficient but less flexible for experimentation, while the exploratory approach is seen as less efficient and attractive. Combining elements of both paradigms may be beneficial to support users' exploratory behavior while guiding them towards effective solutions.\n",
      "http://arxiv.org/abs/2404.05365v1: [ True]\n",
      "The paper discusses the marginalization of indigenous language communities in the face of technological advancements, emphasizing the importance of inclusive NLP innovations that respect their cultural richness. It highlights the NLP progress of indigenous Latin American languages and the challenges and innovations needed for their preservation and development, aiming to bridge the gap between these communities and researchers.\n",
      "http://arxiv.org/abs/2404.05417v1: [ True]\n",
      "This study explores using AI-based analytics to support design education by measuring students' use of space and scale in organizing their design work. Through a research artifact integrating a design analytics dashboard with design instances, instructors reflected on how these analytics could impact design education positively. The findings suggest that indexing design analytics to actual design work instances can help instructors assess and provide feedback effectively, especially in multiscale design contexts.\n",
      "http://arxiv.org/abs/2404.05442v1: [ True]\n",
      "This paper explores using ChatGPT, a generative AI tool, to develop user personas and adaptive interfaces as an alternative to traditional manual processes, showing promising results in improving the design process efficiency. By leveraging the power of Large Language Models, there is potential to streamline the time, effort, and cost associated with user research for user-centered applications.\n",
      "http://arxiv.org/abs/2404.05449v1: [ True]\n",
      "A framework called Reflection on search Trees (RoT) is introduced to enhance the performance of large language models (LLMs) in reasoning and planning tasks by summarizing guidelines from previous search experiences to prevent repeated mistakes. RoT significantly improves LLM performance with tree-search-based prompting methods like BFS and MCTS, and can also benefit non-tree-search-based methods by providing task-specific knowledge derived from search experiences.\n",
      "http://arxiv.org/abs/2404.05569v1: [ True]\n",
      "Recent advancements in large language model agents focus on optimizing agent teams and implementing self-reflection to improve performance in complex tasks. A proposed framework called 360°REA utilizes a multi-agent approach with a 360° performance assessment method to enhance agent capabilities through experience accumulation, showing effectiveness in addressing complex tasks based on experimental results with various datasets.\n",
      "http://arxiv.org/abs/2404.05602v1: [ True]\n",
      "This research proposes an AI-powered cyber incident response system for cloud environments, using AI and ML for Network Traffic Classification and Malware Analysis, achieving high accuracy rates of 90% and 96% respectively with the Random Forest model. The study emphasizes the efficiency and scalability of AI/ML systems in cloud environments, utilizing container technology and cloud-based TPUs and GPUs for managing resource demands and ensuring a robust cyber incident response solution.\n",
      "http://arxiv.org/abs/2404.05720v1: [ True]\n",
      "This work addresses the issue of catastrophic forgetting in zero-shot conditions when fine-tuning pretrained models for summarization tasks across different languages. By using query-key (QK) fine-tuning and a balanced adversarial language classifier, the study aims to improve zero-shot performance and promote language-agnostic representations, as demonstrated through qualitative analyses with openly available code.\n",
      "http://arxiv.org/abs/2404.05779v1: [ True]\n",
      "Data quality is vital for effective AI models, with poor data leading to inaccuracies and potential risks. Current research aims to establish standardized metrics for evaluating data readiness to improve the quality and accuracy of AI training and inference.\n",
      "http://arxiv.org/abs/2404.05874v1: [ True]\n",
      "This study involved a two-week workshop where 13 youth aged 14-15 designed and audited ML-powered applications, leading to increased awareness of algorithmic biases and model design issues among participants. The findings suggest that involving youth in auditing algorithms can enhance their understanding of model functionality and inspire improvements in their own applications, contributing to research on child-computer interaction and learning.\n",
      "http://arxiv.org/abs/2404.05904v1: [ True]\n",
      "Large Language Models (LLMs) are powerful in NLP but can produce inaccurate outputs known as \"hallucinations.\" The Hallucinations Leaderboard quantitatively assesses different models' tendency to create hallucinations across various NLP tasks, helping researchers and practitioners select more reliable models for their needs.\n",
      "http://arxiv.org/abs/2404.05950v1: [ True]\n",
      "Task-Specific Action Correction (TSAC) is proposed to enhance multi-task reinforcement learning (MTRL) by decomposing policy learning into a shared policy (SP) and an action correction policy (ACP), using goal-oriented sparse rewards to improve generalization across tasks. TSAC outperforms existing methods in MTRL by transforming the original multi-objective problem into a single-objective formulation, resulting in improved sample efficiency and effective action execution demonstrated in Meta-World's MT10 and MT50 benchmarks.\n",
      "http://arxiv.org/abs/2404.05990v1: [ True]\n",
      "Rapid advancements in Artificial Intelligence have led to increased power exerted over individuals through automated systems, impacting various aspects of society from government services to information dissemination and economic interactions. Scholars are advocating for a critical examination of these new power dynamics created by AI technologies to ensure proper authority and procedural legitimacy in their use.\n",
      "http://arxiv.org/abs/2404.06063v1: [ True]\n",
      "Aspect-Based Sentiment Analysis (ABSA) is a complex task in natural language processing, with current efforts focusing on specific sub-tasks. A proposed All in One (AiO) model utilizes Generative Pre-trained Transformers (GPTs) for all ABSA sub-tasks, showing effectiveness in handling such tasks even with limited data through two-stage processing.\n",
      "http://arxiv.org/abs/2404.06201v1: [ True]\n",
      "Large Language Models (LLMs) are crucial for improving software engineering tasks by enhancing code understanding, but their collaboration relies heavily on accessing high-quality data, which can be restricted due to commercial and privacy concerns, hindering progress in open-source AI-based software engineering projects. To address this challenge, a governance framework centered on federated learning (FL) is proposed to enable open-source AI models to leverage diverse organizational resources while ensuring data privacy and security, along with guidelines for developers on collaboration aspects such as data requirements, model architecture, updating strategies, and version control.\n",
      "http://arxiv.org/abs/2404.06404v1: [ True]\n",
      "Large Language Models (LLMs) are powerful tools in research, offering benefits like cost-effectiveness and efficiency, but they also pose challenges such as prompt tuning, biases, and subjectivity that need to be addressed. This study explores the potential of LLMs through literature review and experimentation, highlighting successes, limitations, and strategies for mitigating challenges to integrate them into research ethically.\n",
      "http://arxiv.org/abs/2404.06411v1: [ True]\n",
      "Advances in Large Language Models have spurred the development of LLM agents for complex reasoning tasks, emphasizing the importance of benchmarking and evaluation for progress. The AgentQuest framework offers modular benchmarks and new evaluation metrics to track LLM agent progress effectively, highlighting common failure points and enhancing performance through refined architecture, aiming for community collaboration and extension.\n",
      "http://arxiv.org/abs/2404.06664v1: [ True]\n",
      "Current methods for evaluating the multicultural knowledge of large language models struggle to capture the complexity and diversity of cultural norms, potentially perpetuating biases in their benchmarks. An interactive system called CulturalTeaming leverages human-AI collaboration to create challenging evaluation datasets that expose shortcomings in modern LLMs' multicultural proficiency, with accuracy ranging from 37.7% to 72.2% across different LLM families.\n",
      "http://arxiv.org/abs/2404.06777v1: [ True]\n",
      "Integrating AI and federated learning in smart transportation raises concerns about responsible use, crucial for system stability and sustainability. Research on the responsible application of AI and FL in this area is limited, with a need for more thorough exploration and solutions to challenges in developing and implementing responsible FL.\n",
      "http://arxiv.org/abs/2404.06948v1: [ True]\n",
      "The paper introduces a winning solution for the SemEval-2024 Task 6 competition, utilizing a meta-regressor framework of large language models (LLMs) to evaluate and integrate models effectively, achieving top scores on the leader board. By harnessing uncertainty signals from a variety of LLMs, the approach enhances the detection of hallucinations with greater robustness.\n",
      "http://arxiv.org/abs/2404.06954v1: [ True]\n",
      "Dynamic computation methods for Large Language Models have sped up processing by skipping layers through heuristics or predictors, but existing decoding processes assign different computational budgets to samples, leading to instability. The proposed Unified Layer Skipping strategy selects the number of layers to skip based on a speedup ratio, balancing computation skipping to improve inference performance and model throughput for tasks like machine translation and text summarization.\n",
      "http://arxiv.org/abs/2404.07066v1: [ True]\n",
      "This paper investigates how large language models learn different concepts across their layers, with deeper layers handling more complex and abstract concepts. Through a probing technique, it shows that simpler tasks are efficiently classified in shallower layers, while deeper layers are crucial for discerning more complex tasks, impacting our understanding of model learning processes and representations.\n",
      "http://arxiv.org/abs/2404.07142v1: [ True]\n",
      "Software systems are crucial in modern society, but the diversity of software development teams often does not reflect the user base. To ensure inclusive work environments and usable software for diverse populations, efforts are needed to promote software developer diversity and inclusion, especially as technology like AI influences the landscape of software engineering.\n",
      "http://arxiv.org/abs/2404.07143v1: [ True]\n",
      "This work presents a method to extend Transformer-based Large Language Models to process infinitely long inputs efficiently using a new attention technique called Infini-attention, which incorporates compressive memory and different attention mechanisms in a single Transformer block. The approach demonstrates effectiveness in tasks like long-context language modeling and book summarization with 1B and 8B LLMs, enabling fast streaming inference with minimal memory parameters.\n",
      "http://arxiv.org/abs/2404.07413v1: [ True]\n",
      "The JetMoE-8B is a cost-effective Large Language Model trained with minimal resources, showcasing impressive performance and outperforming other models, indicating that LLM training can be more affordable than previously believed. JetMoE-8B utilizes an efficient Sparsely-gated Mixture-of-Experts architecture, with sparsely activated layers reducing inference computation by about 70% compared to other models, and it promotes transparency, collaboration, and advancements in accessible and efficient LLM development.\n",
      "http://arxiv.org/abs/2404.07461v1: [ True]\n",
      "A study examines how hallucination is understood in large language models by analyzing 103 research papers and conducting a survey with 171 NLP and AI experts, revealing a lack of consensus on the term and the need for clear definitions and frameworks in the field. The research highlights the importance of defining hallucination in NLP, emphasizing potential challenges and societal impacts, based on insights gathered from practitioners in the field.\n",
      "http://arxiv.org/abs/2404.07501v1: [ True]\n",
      "Research in business process modeling explores automated generation of process models from data, such as event logs and natural language texts, aiming to reduce costs and improve efficiency. This study investigates using data augmentation techniques from machine learning to enhance accuracy in extracting business process information from text, showing promising results in improving $F_1$ scores for mention and relation extraction. The findings suggest that data augmentation plays a crucial role in advancing machine learning methods for business process model generation from natural language text, offering insights through visualization and analysis of augmented textual data.\n",
      "http://arxiv.org/abs/2404.07678v1: [ True]\n",
      "To thrive, organizations must prioritize innovation and address ethical and sustainability concerns, especially in today's fast-paced environment. This involves integrating ethical and sustainability considerations, particularly in the adoption of emerging technologies like artificial intelligence, to foster innovation cultures in both startups and established enterprises using approaches such as systems thinking.\n",
      "http://arxiv.org/abs/2404.07738v1: [ True]\n",
      "A ResearchAgent powered by a language model is proposed to streamline scientific research by automatically generating and refining research ideas, methods, and experiment designs through iterative processes based on existing literature. By connecting information from academic graphs and entity-centric knowledge stores, as well as incorporating peer review feedback, the ResearchAgent demonstrates effectiveness in producing innovative and valid research ideas across various disciplines.\n",
      "http://arxiv.org/abs/2404.08335v1: [ True]\n",
      "Research has shown that tokenization is essential for creating high-performing language models, as demonstrated by transformers failing to learn the correct distribution without it. By studying transformers on simple data generating processes, it was found that with proper tokenization, they can effectively model sequence probabilities from Markov sources, supporting the practical use of tokenization in language processing.\n",
      "http://arxiv.org/abs/2404.08417v1: [ True]\n",
      "Large language models can now handle complex tasks by recalling information from a pretraining corpus, but concerns arise regarding evolving data needs, such as introducing new data batches or managing user-based data access. AdapterSwap is introduced as a solution to organize knowledge into low-rank adapters, allowing for efficient continual learning and fine-grained control over data access and deletion.\n",
      "http://arxiv.org/abs/2404.08511v1: [ True]\n",
      "This study introduces a novel approach to cross-domain knowledge discovery by deploying specialized multi-AI agents that collaborate to provide comprehensive insights beyond single-domain expertise. Through comparative analysis, the research demonstrates the superior capability of domain-specific multi-AI agents in identifying and bridging knowledge gaps, highlighting the importance of collaborative AI in driving innovation and advancing cross-disciplinary research and applications.\n",
      "http://arxiv.org/abs/2404.08570v1: [ True]\n",
      "The CRITICAL framework enhances autonomous vehicle training and testing by generating diverse scenarios targeting specific learning gaps through real-world traffic dynamics and optional Large Language Model analysis. By establishing a closed feedback loop between data generation and training, CRITICAL improves learning rates, system performance, and safety resilience, showing potential to enhance AV systems' robustness and accelerate their development.\n",
      "http://arxiv.org/abs/2404.08727v1: [ True]\n",
      "This study evaluates the resource utilization and accuracy of Large Language Models (LLMs) in interpreting natural language queries against traditional SQL in relational database management systems, finding that even small LLMs incur significant energy overhead, making them environmentally unfriendly for database queries. The findings caution against replacing relational databases with LLMs due to their substantial resource utilization.\n",
      "http://arxiv.org/abs/2404.08743v1: [ True]\n",
      "Instructors use collaborative learning activities like Peer Instruction to engage students, but diverse mental models and ineffective collaboration can hinder success. VizGroup is an AI tool that helps programming instructors oversee real-time student collaboration during large courses by recommending event specifications based on collaboration metrics, leading to improved monitoring and notification strategies.\n",
      "http://arxiv.org/abs/2404.08811v1: [ True]\n",
      "The surge in demand for Machine Learning and Artificial Intelligence applications is straining the technology infrastructure, leading to unsustainable spending trends and hindering innovation. Addressing these challenges requires significant advancements in supercomputing, AI training approaches, hardware design, and energy efficiency to reduce barriers to entry for training large language models.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(new_preds)):\n",
    "    if new_preds[i] == True:\n",
    "        paper_id = new_ids[i][\"paper_id\"]\n",
    "        summary = new_ids[i][\"concise_summary\"]\n",
    "        print(f\"{paper_id}: {new_preds[i]}\\n{summary}\")\n",
    "        formatted.append({\"id\": paper_id, \"summary\": summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e31e9-9ae4-404a-b5ae-8ada5fecc091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
